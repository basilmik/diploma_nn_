{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "K5feZBXDqSyO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5feZBXDqSyO",
    "outputId": "f8fa2bd4-7df7-40c6-97a6-3a71739035a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290e9d45-e28b-4603-9058-3260849a1668",
   "metadata": {
    "id": "290e9d45-e28b-4603-9058-3260849a1668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from truly_plastic_p4.ipynb\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "from truly_plastic_p4 import plastic_nn\n",
    "from truly_plastic_p4 import input_layer\n",
    "from truly_plastic_p4 import layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7cea6a06-3557-4e61-a769-420d9de43ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class replay_memory():\n",
    "    def __init__(s, maxlen):\n",
    "        s.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(s, transition):\n",
    "        s.memory.append(transition)\n",
    "\n",
    "    def sample(s, sample_size):\n",
    "        return random.sample(s.memory, sample_size)\n",
    "\n",
    "    def __len__(s):\n",
    "        return len(s.memory)\n",
    "\n",
    "\n",
    "\n",
    "class DQN():\n",
    "   \n",
    "    def __init__(s, ct = 0, tag=0, path = r'test', \n",
    "                 game_name = 'MountainCar-v0', \n",
    "                  \n",
    "                 mini_batch_size = 32,  num_divisions = 1, \n",
    "                 replay_memory_size = 100000, \n",
    "                 network_sync_rate = 50000, discount_factor_g = 0.9):\n",
    "        \n",
    "        if (ct == 0):\n",
    "            ct = datetime.datetime.now()\n",
    "            ct = str(ct)\n",
    "            ct = ct.replace(\":\", \"-\")\n",
    "            ct = ct.replace(\" \", \"_\")\n",
    "            ct = ct[:-7]\n",
    "\n",
    "        s.ct = ct\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory_pics = os.path.join(current_directory, s.ct)\n",
    "        \n",
    "        final_directory_pics = os.path.join(final_directory_pics, 'pics')\n",
    "        if not os.path.exists(final_directory_pics):\n",
    "            os.makedirs(final_directory_pics)\n",
    "        \n",
    "        if (tag != 0):\n",
    "            s.set_tag(tag)\n",
    "\n",
    "        s.game_name = game_name\n",
    "        s.discount_factor_g = discount_factor_g\n",
    "         \n",
    "        s.mini_batch_size = mini_batch_size \n",
    "        s.num_divisions = num_divisions\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        s.lin_spaces = []\n",
    "        env = gym.make(s.game_name)\n",
    "        obs_space = env.observation_space\n",
    "\n",
    "        for i in range(obs_space.shape[0]):\n",
    "            s.lin_spaces.append(np.linspace(env.observation_space.low[i], env.observation_space.high[i], s.num_divisions))\n",
    "\n",
    "        env.close()\n",
    "\n",
    "        \n",
    "        s.is_desrcete = True if num_divisions == 1 else False\n",
    "        \n",
    "        s.replay_memory_size =  replay_memory_size \n",
    "        s.network_sync_rate = network_sync_rate\n",
    "        \n",
    "    def set_tag(s, tag):\n",
    "        s.tag = tag\n",
    "        s.path = s.ct+'/'+s.tag\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory = os.path.join(current_directory, s.path)\n",
    "        if not os.path.exists(final_directory):\n",
    "            os.makedirs(final_directory)\n",
    "        \n",
    "   \n",
    "    def plot_progress(self, rewards_per_episode_, epsilon_history_):\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('reward')\n",
    "        plt.plot(rewards_per_episode_)\n",
    "\n",
    "        plt.savefig(f'{self.path}/info_rew_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.savefig(f'{self.ct}/pics/info_rew_{self.tag}.png')\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('epsilon')\n",
    "        plt.plot(epsilon_history_)\n",
    "        plt.savefig(f'{self.path}/info_eps_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    def save_info(s, info):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\")   \n",
    "       \n",
    "        f.write(\"data {}\\n\".format(s.ct))\n",
    "        f.write(\"tag {}\\n\".format(s.tag))\n",
    "        f.write(\"game_name {}\\n\".format(s.game_name))\n",
    "        f.write(\"reward discount factor {}\\n\".format(s.discount_factor_g))\n",
    "        f.write(\"minibatch size {}\\n\".format(s.mini_batch_size))\n",
    "        f.write(\"num divisions{}\\n\".format(s.num_divisions))\n",
    "        f.write(\"replay memory size {}\\n\".format(s.replay_memory_size))\n",
    "        f.write(\"network sync rate {}\\n\".format(s.network_sync_rate))\n",
    "        f.write(\"info {}\\n\".format(info))\n",
    "                         \n",
    "        f.close()\n",
    "    \n",
    "    def add_log_data(s, data):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n",
    "        \n",
    "    def save_reward_data(s, data):\n",
    "        file_path = f'{s.path}/rewards_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def run_one_epoch(s, i):\n",
    "        s.state = s.env.reset()[0]\n",
    "        s.terminated = False\n",
    "        s.truncated = False\n",
    "        s.rewards = 0\n",
    "\n",
    "        while(not s.terminated and s.rewards < 300):\n",
    "            if random.random() < s.epsilon:\n",
    "                s.action = env.action_space.sample()\n",
    "            else:\n",
    "                s.res = s.policy_dqn.forward(s.state_to_dqn_input(s.state))\n",
    "                s.action = s.res.argmax().item()\n",
    "\n",
    "            s.new_state, s.reward, s.terminated, s.truncated, _ = s.env.step(s.action)\n",
    "            s.rewards += s.reward\n",
    "            s.memory.append((s.state, s.action, s.new_state, s.reward, s.terminated))\n",
    "            s.state = s.new_state\n",
    "            \n",
    "            s.step_count+=1\n",
    " \n",
    "        \n",
    "        if(s.terminated):\n",
    "            s.goal_reached = True\n",
    "\n",
    "        s.rewards_per_episode.append(s.rewards)\n",
    "        \n",
    "        \n",
    "        if(i!=0 and i%1000==0): # Graph training progress\n",
    "            print(f'Episode {i} Epsilon {s.epsilon}')\n",
    "            \n",
    "            s.policy_dqn.save(f'{s.path}/mc_policy_{i}'.format(s.path, i))\n",
    "            s.add_log_data(f'Episode {i} Epsilon {s.epsilon}')\n",
    "            \n",
    "            s.plot_progress(s.rewards_per_episode, s.epsilon_history)\n",
    "\n",
    "        \n",
    "        if s.rewards > s.best_rewards:\n",
    "            s.best_rewards = s.rewards\n",
    "            \n",
    "            print(f'Best rewards so far: {s.best_rewards}')\n",
    "            s.add_log_data(f'Best rewards so far: {s.best_rewards}')\n",
    "            s.policy_dqn.save(f'{s.path}/mc_policy_{i}'.format(s.path, i))\n",
    "            s.policy_dqn.save(f'{s.path}/mc_policy_best'.format(s.path))\n",
    "            \n",
    "\n",
    "        if len(s.memory) > s.mini_batch_size and s.goal_reached: # if enough experience has been collected\n",
    "\n",
    "            s.mini_batch = s.memory.sample(s.mini_batch_size)\n",
    "            s.optimize(s.mini_batch, s.policy_dqn, s.target_dqn)\n",
    "\n",
    "            # Decay epsilon\n",
    "            s.epsilon = max(s.epsilon - 1/s.episodes, 0.01) \n",
    "            s.epsilon_history.append(s.epsilon)\n",
    "\n",
    "            # Copy policy network to target network after a certain number of steps\n",
    "            if s.step_count > s.network_sync_rate:\n",
    "                s.target_dqn = s.policy_dqn.deep_copy()\n",
    "                s.step_count = 0\n",
    "\n",
    "        return s.rewards\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def init_train_data(s, policy_dqn, episodes):\n",
    "        s.policy_dqn = policy_dqn\n",
    "        s.target_dqn = plastic_nn()\n",
    "        s.target_dqn = policy_dqn.deep_copy()\n",
    "\n",
    "        \n",
    "        s.epsilon = 1 # 1 = 100% random actions\n",
    "        s.memory = replay_memory(s.replay_memory_size)\n",
    "\n",
    "        s.rewards_per_episode = []\n",
    "        s.epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        s.step_count = 0\n",
    "        s.goal_reached = False\n",
    "        s.best_rewards = -200\n",
    "\n",
    "        s.episodes = episodes \n",
    "        s.env = gym.make(s.game_name)\n",
    "\n",
    "    \n",
    "    def train(s, episodes):\n",
    "        #s.init_train_data(policy_dqn, episodes)\n",
    "        \n",
    "\n",
    "        # main training\n",
    "        for i in range(s.episodes+1):\n",
    "            s.run_one_epoch(i)\n",
    "        \n",
    "        # # done\n",
    "        # s.post_train()\n",
    "\n",
    "\n",
    "    def post_train(s):\n",
    "                \n",
    "        s.env.close()\n",
    "        s.policy_dqn.save(f'{s.path}/mc_policy_last_{s.tag}'.format(s.path, s.tag))\n",
    "        \n",
    "        if (s.best_rewards == -200): #update best\n",
    "            s.policy_dqn.save(f'{s.path}/mc_policy_best'.format(s.path))\n",
    "            \n",
    "        s.save_reward_data(s.rewards_per_episode)\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        target_q_list = []\n",
    "        input_list = []\n",
    "        \n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor_g * target_dqn.forward(self.state_to_dqn_input(new_state)).max()\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            state_dsc = np.asarray(self.state_to_dqn_input(state))\n",
    "            input_list.append(state_dsc)\n",
    "            \n",
    "            target_q = target_dqn.forward(state_dsc)\n",
    "            \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target            \n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        #BACKPOP AND UPDATE on minibatch\n",
    "        x = np.asarray(input_list)\n",
    "        x = x[:, :, 0]\n",
    "        x = x.T\n",
    "\n",
    "        y = np.asarray(target_q_list)\n",
    "        y = y[:, :, 0]\n",
    "        y = y.T\n",
    "\n",
    "        policy_dqn.learn_one(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_dqn_input(s, state):\n",
    "        \n",
    "        if (not s.is_desrcete):\n",
    "            return np.asarray(state) #[[state[0]], [state[1]], [state[2]], [state[3]]])\n",
    "        else:\n",
    "            d_state = []\n",
    "            for i in range(state.shape[0]):\n",
    "                dig = np.digitize(state[i], s.lin_spaces[i])\n",
    "                d_state.append(np.asarray([dig]))\n",
    "        \n",
    "            return np.asarray(d_state)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def test(self, policy_dqn, episodes, render = False):\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "        \n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        done_count = 0\n",
    "        medium_reward = 0\n",
    "        reward_list = []\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            state, info = env.reset() \n",
    "            done = False\n",
    "            truncated = False \n",
    "            rewards = 0\n",
    "\n",
    "            while(not done and not truncated):\n",
    "                state = self.state_to_dqn_input(state)\n",
    "\n",
    "                res = policy_dqn.forward(state)\n",
    "\n",
    "                action = res.argmax().item()\n",
    "\n",
    "                state, reward, done, truncated, _ = env.step(action)\n",
    "                rewards+=reward\n",
    "                if (truncated):\n",
    "                    done_count += 1\n",
    "                    break\n",
    "\n",
    "            medium_reward += rewards\n",
    "            reward_list.append(rewards)\n",
    "                \n",
    "\n",
    "        \n",
    "        env.close()\n",
    "        medium_reward = medium_reward / episodes\n",
    "        return done_count*100.0/episodes, medium_reward, reward_list\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb6427-5c7b-4b3c-a146-0c79966845bc",
   "metadata": {},
   "source": [
    "## alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4e411266-8496-456b-a99d-d0e82f0441b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basil\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\basil\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basil\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "learning_rate_a = 0.2\n",
    "game_name = \"CartPole-v0\"\n",
    "env = gym.make(game_name)\n",
    "in_states = env.observation_space.shape[0]\n",
    "out_actions = env.action_space.n\n",
    "env.close()\n",
    "\n",
    "\n",
    "h1_nodes = 12\n",
    "h2_nodes = 12\n",
    "\n",
    "a_type1 = 'ReLU'\n",
    "a_type2 = 'Linear'\n",
    "\n",
    "\n",
    "num_of_nn = 2\n",
    "\n",
    "\n",
    "dqn_s = []\n",
    "\n",
    "for i in range(num_of_nn):\n",
    "    print(i)\n",
    "    layers_net = [input_layer(in_states), \n",
    "    layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type=a_type1), \n",
    "    layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type=a_type1), \n",
    "    layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type=a_type2)]\n",
    "    \n",
    "    policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "    policy_dqn.append_layers(layers_net)\n",
    "    \n",
    "    dqn_test = DQN(game_name = game_name, mini_batch_size = 64, num_divisions = 1)\n",
    "    dqn_test.set_tag(str(i))\n",
    "    \n",
    "    dqn_s.append(dqn_test)\n",
    "    \n",
    "episodes = 2005\n",
    "\n",
    "for i in range (num_of_nn):\n",
    "    print(i)\n",
    "    dqn_s[i].init_train_data(policy_dqn, episodes)\n",
    "\n",
    "mean_results = []\n",
    "\n",
    "for e in range(episodes +1):\n",
    "    summ = 0\n",
    "    for i in range (num_of_nn):\n",
    "        summ += dqn_s[i].run_one_epoch(e)\n",
    "    mean_results.append(summ/num_of_nn)\n",
    "    #print(res)\n",
    "        \n",
    "for i in range (num_of_nn):\n",
    "    print(i)\n",
    "    dqn_s[i].post_train()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
