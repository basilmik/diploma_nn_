{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069f34c-0d32-40f6-ade7-f4c28f8dbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow.contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fd0fed-f0c7-4a1f-ba37-567e03e1dcc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlayers\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrappers\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgent\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pylab import *\n",
    "# import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow.contrib.layers as layers\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, input_size=4, hidden_size=2, gamma=0.95,\n",
    "                 action_size=2, lr=0.1, dir='tmp/trial/'):\n",
    "        # call the cartpole simulator from OpenAI gym package\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        # If you wish to save the simulation video, simply uncomment the line below\n",
    "        # self.env = wrappers.Monitor(self.env, dir, force=True, video_callable=self.video_callable)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        # save the hyper parameters\n",
    "        self.params = self.__dict__.copy()\n",
    "\n",
    "        # inputs to the controller\n",
    "        self.input_pl = tf.placeholder(tf.float32, [None, input_size])\n",
    "        self.action_pl = tf.placeholder(tf.int32, [None])\n",
    "        self.reward_pl = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # Here we use a single layered neural network as controller, which proved to be sufficient enough.\n",
    "        # More complicated ones can be plugged in as well.\n",
    "        # hidden_layer = layers.fully_connected(self.input_pl,\n",
    "        #                                      hidden_size,\n",
    "        #                                      biases_initializer=None,\n",
    "        #                                      activation_fn=tf.nn.relu)\n",
    "        # hidden_layer = layers.fully_connected(hidden_layer,\n",
    "        #                                       hidden_size,\n",
    "        #                                       biases_initializer=None,\n",
    "        #                                       activation_fn=tf.nn.relu)\n",
    "        self.output = layers.fully_connected(self.input_pl,\n",
    "                                             action_size,\n",
    "                                             biases_initializer=None,\n",
    "                                             activation_fn=tf.nn.softmax)\n",
    "\n",
    "        # responsible output\n",
    "        self.one_hot = tf.one_hot(self.action_pl, action_size)\n",
    "        self.responsible_output = tf.reduce_sum(self.output * self.one_hot, axis=1)\n",
    "\n",
    "        # loss value of the network\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_output) * self.reward_pl)\n",
    "\n",
    "        # get all network variables\n",
    "        variables = tf.trainable_variables()\n",
    "        self.variable_pls = []\n",
    "        for i, var in enumerate(variables):\n",
    "            self.variable_pls.append(tf.placeholder(tf.float32))\n",
    "\n",
    "        # compute the gradient values\n",
    "        self.gradients = tf.gradients(self.loss, variables)\n",
    "\n",
    "        # update network variables\n",
    "        solver = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "        # solver = tf.train.MomentumOptimizer(learning_rate=self.lr,momentum=0.95)\n",
    "        self.update = solver.apply_gradients(zip(self.variable_pls, variables))\n",
    "\n",
    "\n",
    "    def video_callable(self, episode_id):\n",
    "        # display the simulation trajectory every 50 epoch\n",
    "        return episode_id % 50 == 0\n",
    "\n",
    "    def next_action(self, sess, feed_dict, greedy=False):\n",
    "        \"\"\"Pick an action based on the current state.\n",
    "        Args:\n",
    "        - sess: a tensorflow session\n",
    "        - feed_dict: parameter for sess.run()\n",
    "        - greedy: boolean, whether to take action greedily\n",
    "        Return:\n",
    "            Integer, action to be taken.\n",
    "        \"\"\"\n",
    "        ans = sess.run(self.output, feed_dict=feed_dict)[0]\n",
    "        if greedy:\n",
    "            return ans.argmax()\n",
    "        else:\n",
    "            return np.random.choice(range(self.action_size), p=ans)\n",
    "\n",
    "    def show_parameters(self):\n",
    "        \"\"\"Helper function to show the hyper parameters.\"\"\"\n",
    "        for key, value in self.params.items():\n",
    "            print(key, '=', value)\n",
    "\n",
    "def discounted_reward(rewards, gamma):\n",
    "    \"\"\"Compute the discounted reward.\"\"\"\n",
    "    ans = np.zeros_like(rewards)\n",
    "    running_sum = 0\n",
    "    # compute the result backward\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        running_sum = running_sum * gamma + rewards[i]\n",
    "        ans[i] = running_sum\n",
    "    return ans\n",
    "\n",
    "def one_trial(agent, sess, grad_buffer, reward_itr, i, render = False):\n",
    "    '''\n",
    "    this function does follow things before a trial is done:\n",
    "    1. get a sequence of actions based on the current state and a given control policy\n",
    "    2. get the system response of a given action\n",
    "    3. get the instantaneous reward of this action\n",
    "    once a trial is done:\n",
    "    1. get the \"long term\" value of the controller\n",
    "    2. get the gradient of the controller\n",
    "    3. update the controller variables\n",
    "    4. output the state history\n",
    "    '''\n",
    "\n",
    "    # reset the environment\n",
    "    s = agent.env.reset()\n",
    "    for idx in range(len(grad_buffer)):\n",
    "        grad_buffer[idx] *= 0\n",
    "    state_history = []\n",
    "    reward_history = []\n",
    "    action_history = []\n",
    "    current_reward = 0\n",
    "\n",
    "    while True:\n",
    "\n",
    "        feed_dict = {agent.input_pl: [s]}\n",
    "        # update the controller deterministically\n",
    "        greedy = False\n",
    "        # get the controller output under a given state\n",
    "        action = agent.next_action(sess, feed_dict, greedy=greedy)\n",
    "        # get the next states after taking an action\n",
    "        snext, r, done, _ = agent.env.step(action)\n",
    "        if render and i % 50 == 0:\n",
    "            agent.env.render()\n",
    "        current_reward += r\n",
    "        state_history.append(s)\n",
    "        reward_history.append(r)\n",
    "        action_history.append(action)\n",
    "        s = snext\n",
    "\n",
    "        if done:\n",
    "\n",
    "            # record how long it has been balancing when the simulation is done\n",
    "            reward_itr += [current_reward]\n",
    "\n",
    "            # get the \"long term\" rewards by taking decay parameter gamma into consideration\n",
    "            rewards = discounted_reward(reward_history, agent.gamma)\n",
    "\n",
    "            # normalizing the reward makes training faster\n",
    "            rewards = (rewards - np.mean(rewards)) / np.std(rewards)\n",
    "\n",
    "            # compute network gradients\n",
    "            feed_dict = {\n",
    "                agent.reward_pl: rewards,\n",
    "                agent.action_pl: action_history,\n",
    "                agent.input_pl: np.array(state_history)\n",
    "            }\n",
    "            episode_gradients = sess.run(agent.gradients,feed_dict=feed_dict)\n",
    "            for idx, grad in enumerate(episode_gradients):\n",
    "                grad_buffer[idx] += grad\n",
    "\n",
    "            # apply gradients to the network variables\n",
    "            feed_dict = dict(zip(agent.variable_pls, grad_buffer))\n",
    "            sess.run(agent.update, feed_dict=feed_dict)\n",
    "\n",
    "            # reset the buffer to zero\n",
    "            for idx in range(len(grad_buffer)):\n",
    "                grad_buffer[idx] *= 0\n",
    "            break\n",
    "\n",
    "    return state_history\n",
    "\n",
    "def animate_itr(i,*args):\n",
    "    '''animantion of each training epoch'''\n",
    "    agent, sess, grad_buffer, reward_itr, sess, grad_buffer, agent, obt_itr, render = args\n",
    "    #\n",
    "    state_history = one_trial(agent, sess, grad_buffer, reward_itr, i, render)\n",
    "    xlist = [range(len(reward_itr))]\n",
    "    ylist = [reward_itr]\n",
    "    for lnum, line in enumerate(lines_itr):\n",
    "        line.set_data(xlist[lnum], ylist[lnum])  # set data for each line separately.\n",
    "\n",
    "    if len(reward_itr) % obt_itr == 0:\n",
    "        x_mag = 2.4\n",
    "        y_mag = 30 * 2 * math.pi / 360\n",
    "        # normalize to (-1,1)\n",
    "        xlist = [np.asarray(state_history)[:,0] / x_mag]\n",
    "        ylist = [np.asarray(state_history)[:,2] / y_mag]\n",
    "        lines_obt.set_data(xlist, ylist)\n",
    "        tau = 0.02\n",
    "        time_text_obt.set_text('physical time = %6.2fs' % (len(xlist[0])*tau))\n",
    "\n",
    "    return (lines_itr,) + (lines_obt,) + (time_text_obt,)\n",
    "\n",
    "\n",
    "def get_fig(max_epoch):\n",
    "    fig = plt.figure()\n",
    "    ax_itr = axes([0.1, 0.1, 0.8, 0.8])\n",
    "    ax_obt = axes([0.5, 0.2, .3, .3])\n",
    "\n",
    "    # able to display multiple lines if needed\n",
    "    global lines_obt, lines_itr, time_text_obt\n",
    "    lines_itr = []\n",
    "    lobj = ax_itr.plot([], [], lw=1, color=\"blue\")[0]\n",
    "    lines_itr.append(lobj)\n",
    "    lines_obt = []\n",
    "\n",
    "    ax_itr.set_xlim([0, max_epoch])\n",
    "    ax_itr.set_ylim([0, 220])#([0, max_reward])\n",
    "    ax_itr.grid(False)\n",
    "    ax_itr.set_xlabel('trainig epoch')\n",
    "    ax_itr.set_ylabel('reward')\n",
    "\n",
    "    time_text_obt = []\n",
    "    ax_obt.set_xlim([-1, 1])\n",
    "    ax_obt.set_ylim([-1, 1])\n",
    "    ax_obt.set_xlabel('cart position')\n",
    "    ax_obt.set_ylabel('pole angle')\n",
    "    lines_obt = ax_obt.plot([], [], lw=1, color=\"red\")[0]\n",
    "    time_text_obt = ax_obt.text(0.05, 0.9, '', fontsize=13, transform=ax_obt.transAxes)\n",
    "    return fig, ax_itr, ax_obt, time_text_obt\n",
    "\n",
    "\n",
    "def main():\n",
    "    obt_itr = 10\n",
    "    max_epoch = 3000\n",
    "    # whether to show the pole balancing animation\n",
    "    render = True\n",
    "    dir = 'tmp/trial/'\n",
    "\n",
    "    # set up figure for animation\n",
    "    fig, ax_itr, ax_obt, time_text_obt = get_fig(max_epoch)\n",
    "    agent = Agent(hidden_size=24, lr=0.2, gamma=0.95, dir=dir)\n",
    "    agent.show_parameters()\n",
    "\n",
    "    # tensorflow initialization for neural network controller\n",
    "    tfconfig = tf.ConfigProto()\n",
    "    tfconfig.gpu_options.allow_growth=True\n",
    "    sess = tf.Session(config=tfconfig)\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    grad_buffer = sess.run(tf.trainable_variables())\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    global reward_itr\n",
    "    reward_itr = []\n",
    "    args = [agent, sess, grad_buffer, reward_itr, sess, grad_buffer, agent, obt_itr, render]\n",
    "    # run the optimization and output animation\n",
    "    ani = animation.FuncAnimation(fig, animate_itr,fargs=args)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()\n",
    "\n",
    "# Set up formatting for the movie files\n",
    "# print('saving animation...')\n",
    "# Writer = animation.writers['ffmpeg']\n",
    "# writer = Writer(fps=100, metadata=dict(artist='Me'), bitrate=1800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
