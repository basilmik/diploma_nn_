{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa1c7b5-8a91-4a55-80f1-3e92beef3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc6907-087e-4480-8398-7e61559d96f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## activation and d_activation funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad536855-8dd3-4cad-8ddb-c4fdbac5bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    res = 1 / (1 + np.exp(-x))\n",
    "    return res\n",
    "\n",
    "def d_Sigmoid(x):\n",
    "    y = Sigmoid(x) * (1 - Sigmoid(x))\n",
    "    return y\n",
    "\n",
    "def ReLU(x):\n",
    "    x = np.maximum(0, x)\n",
    "    return x\n",
    "    \n",
    "def d_ReLU(x):\n",
    "    y=x.copy()\n",
    "    y[y<=0] = 0\n",
    "    y[y>0] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def Linear(x):\n",
    "    return x\n",
    "\n",
    "def d_Linear(x):\n",
    "    y = np.ones(shape=(x.shape), dtype = x.dtype)\n",
    "    return y\n",
    "\n",
    "activations_dict = {\n",
    "'Sigmoid': [Sigmoid, d_Sigmoid],\n",
    "'ReLU': [ReLU, d_ReLU], \n",
    "'Linear': [Linear, d_Linear]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4841a1-f502-45b1-b0de-81d5b4b93420",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## input layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a412117-9900-49b3-924b-df23c6998e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class input_layer:\n",
    "    def __init__(s, size):\n",
    "        s.size = size\n",
    "        #s.values = np.zeros(shape=(size), dtype = float)\n",
    "    \n",
    "    def add_neuron(s):\n",
    "        #add_v = np.zeros(shape=(n_of_neurons), dtype=float)\n",
    "        #s.values = np.concatenate((s.values, add_v.T))\n",
    "        s.size += 1\n",
    "\n",
    "    def delete_neuron(s, neuron_number):\n",
    "        s.size-=1\n",
    "        \n",
    "    def delete_new_prev_size(s):     \n",
    "        return\n",
    "\n",
    "\n",
    "    def add_new_prev_size(s):     \n",
    "        return\n",
    "\n",
    "        \n",
    "    def print_info(s):\n",
    "        print(\"IN LAYER\\nsize: \", s.size)\n",
    "\n",
    "    \n",
    "    def print_pic(s):\n",
    "        print_size = min(2, s.size)\n",
    "\n",
    "        for i in range(print_size): \n",
    "            print(\"| |\\t\", end='')\n",
    "        print(\"\")\n",
    "        for i in range(print_size):\n",
    "            print(\" v \\t\", end='')\n",
    "        print(\"\")\n",
    "        for i in range(print_size):\n",
    "            print(' @\\t', end='')\n",
    "        print (\"--\", format(s.size, ' 5d') , \"--\\t\", end='')\n",
    "\n",
    "    def forward(s, x, to_print = False):\n",
    "        s.values = x\n",
    "        return x\n",
    "        \n",
    "    def forward_nu(s, x):\n",
    "        return x\n",
    "\n",
    "    def get_info(s):\n",
    "        return s.size\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0f825-12e4-4500-b273-9b4c4a74a94b",
   "metadata": {},
   "source": [
    "## layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42278612-f311-452d-93eb-c6ce6785a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(s, lr = 0.1, prev_size = 2, my_size=2, activation_type = \"Sigmoid\", weights = None, bias = None, optimizer = \"SGD\", beta = 0.9):\n",
    "        s.lr = lr\n",
    "        s.size = my_size\n",
    "        s.prev_size = prev_size\n",
    "        if (np.all(weights == None)):\n",
    "            #s.weights = np.random.random((prev_size, s.size))\n",
    "            s.weights = np.random.random((s.size, prev_size))\n",
    "        else:\n",
    "            s.weights = weights.copy()\n",
    "            \n",
    "        if (np.all(bias == None)):\n",
    "            #s.bias = np.random.random((s.size))\n",
    "            s.bias = np.random.random((s.size))\n",
    "        else:\n",
    "            s.bias = bias.copy()\n",
    "            \n",
    "        s.activation_type = activation_type\n",
    "        funcs = activations_dict.get(activation_type)\n",
    "        s.activation_f = funcs[0]\n",
    "        s.d_activation_f = funcs[1]\n",
    "\n",
    "        opt_list = ['SGD', 'SGDwM']\n",
    "        if optimizer in opt_list:\n",
    "            s.optimizer = optimizer\n",
    "        else:\n",
    "            print('no such optimizer, available are: ')\n",
    "            for each in opt_list:\n",
    "                print(each)\n",
    "            return\n",
    "                \n",
    "        if (s.optimizer == 'SGDwM'):\n",
    "            s.Vdw = np.zeros(shape=(prev_size, s.size))\n",
    "            s.Vdb = np.zeros(shape=(s.size))\n",
    "            s.beta = beta\n",
    "\n",
    "    def activate(s, x):\n",
    "        return s.activation_f(x)\n",
    "        \n",
    "    def d_activate(s, x):\n",
    "        return s.d_activation_f(x)  \n",
    "\n",
    "    def forward(s, inputs, to_print = False):\n",
    "        s.input = np.asarray(inputs)\n",
    "        s.neurons = np.dot(s.input, s.weights)\n",
    "        \n",
    "        if (to_print): \n",
    "            print('x*w ', s.neurons)\n",
    "            \n",
    "        s.neurons += s.bias\n",
    "        \n",
    "        if (to_print): \n",
    "            print('+ bias ', s.neurons)\n",
    "        s.neurons_activated = s.activate(s.neurons)\n",
    "\n",
    "        if (to_print): \n",
    "            print('s.neurons_activated ', s.neurons_activated)\n",
    "            \n",
    "        return s.neurons_activated\n",
    "\n",
    "    def forward_nu(s, inputs):       \n",
    "        input_data = np.asarray(inputs)\n",
    "        neurons = np.dot(input_data, s.weights)\n",
    "        neurons += s.bias\n",
    "        neurons_activated = s.activate(neurons)       \n",
    "        return neurons_activated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def backprop(s, layer_error):\n",
    "        s.delta = layer_error * s.d_activate(s.neurons)\n",
    "        s.prev_layer_error = np.dot(s.delta, s.weights.T)\n",
    "        return s.prev_layer_error\n",
    "        \n",
    "    def update_weights(s):\n",
    "        if (s.optimizer == \"SGD\"):\n",
    "            s.input_t = s.input.T\n",
    "            s.x = s.input_t#.reshape((s.input_t.shape[0], 1))\n",
    "            s.d = s.delta#.reshape((1, s.delta.shape[0]))\n",
    "            dw = np.dot(s.x, s.d)\n",
    "            db = s.delta\n",
    "            s.weights -= s.lr * dw\n",
    "            s.bias -=  s.lr * db\n",
    "            \n",
    "        elif (s.optimizer==\"SGDwM\"):\n",
    "            \n",
    "            s.input_t = s.input.T\n",
    "            #print(s.input_t.shape[1])\n",
    "            s.x = s.input_t #.reshape((s.input_t.shape[0], 1))\n",
    "            s.d = s.delta#.reshape((1, s.delta.shape[0]))\n",
    "            \n",
    "            dw = np.dot(s.x, s.d)\n",
    "            db = s.delta\n",
    "            \n",
    "            s.Vdw = s.beta*s.Vdw + (1-s.beta)*dw\n",
    "            s.Vdb = s.beta*s.Vdb + (1-s.beta)*db\n",
    "\n",
    "            s.weights -= s.lr*s.Vdw\n",
    "            s.bias -=  s.lr * s.Vdb\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            print(\"NO SUCH OPTIMIZER!\")\n",
    "            return\n",
    "        \n",
    "    def print_info(s):\n",
    "        print(\"my size: \", s.size)\n",
    "        print(\"w: \", s.weights, \"\\n\")\n",
    "        print(\"b: \", s.bias, \"\\n\")\n",
    "\n",
    "              \n",
    "    def print_pic(s):\n",
    "        print_size = min(2, s.size)\n",
    "        print(\"\\nâ•»...\\nv...\")\n",
    "        for i in range(print_size):\n",
    "            print('O\\t', end='')\n",
    "        print (\"--\", format(s.size, ' 5d') , \"--\\t\", end='')\n",
    "\n",
    "\n",
    "\n",
    "    def correct_prev_size(s, new_prev_szie):\n",
    "        dif = new_prev_szie - s.prev_size\n",
    "        if dif > 0: # new prev is greater\n",
    "            for i in range(dif):\n",
    "                s.add_new_prev_size()\n",
    "        elif dif < 0:\n",
    "            dif*=-1\n",
    "            for i in range(dif):\n",
    "                s.delete_new_prev_size()\n",
    "        s.prev_size = new_prev_szie\n",
    "\n",
    "\n",
    "    def delete_neuron(s, neuron_number):\n",
    "        s.weights = np.delete(s.weights, neuron_number, axis = 1)\n",
    "        s.bias = np.delete(s.bias, neuron_number, axis = 0)\n",
    "        s.size-=1\n",
    "        \n",
    "    def delete_new_prev_size(s):     \n",
    "        s.weights = np.delete(s.weights, 0, axis = 0)\n",
    "        s.prev_size -=1\n",
    "\n",
    "\n",
    "    def add_neuron(s):     \n",
    "        add_w = np.zeros(shape=(s.prev_size, 1), dtype=float) + 0.1 # np.random.random((s.prev_size, n_of_neurons)) #\n",
    "        s.weights = np.concatenate((s.weights.T, add_w.T)).T\n",
    "        add_b = np.zeros(shape=(1), dtype=float) + 0.1 \n",
    "        s.bias = np.concatenate((s.bias, add_b))\n",
    "        s.size+=1\n",
    "\n",
    "    def add_new_prev_size(s):     \n",
    "        add_w = np.zeros(shape=(1, s.size), dtype=float) + 0.1\n",
    "        s.weights = np.concatenate((s.weights, add_w))\n",
    "        s.prev_size += 1\n",
    "\n",
    "    def get_info(s):\n",
    "        return s.prev_size, s.size, s.weights, s.bias, s.activation_type, s.lr, s.optimizer, s.beta\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de2bc3-77ea-4d58-b1d4-d33b4863d230",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## plastic_nn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5958be-6ee5-4b92-9c34-7a695b81f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class plastic_nn:\n",
    "    def __init__(s):\n",
    "        s.layers = []\n",
    "        s.n_of_layers = 0\n",
    "        s.name = 'noname'\n",
    "        pass\n",
    "\n",
    "    def give_name(s, name):\n",
    "        s.name = name\n",
    "        \n",
    "    def set_num_of_layers(s, num):\n",
    "        s.n_of_layers = num\n",
    "        \n",
    "    def deep_copy(s):\n",
    "        return copy.deepcopy(s)\n",
    "\n",
    "    \n",
    "    def forward(s, data, to_print = False):\n",
    "        for lay in s.layers:\n",
    "            data = lay.forward(data, to_print)\n",
    "        s.last_result = data\n",
    "        return s.last_result\n",
    "        \n",
    "    def forward_print(s, data, to_print = False):\n",
    "        print('in: ',data)\n",
    "        cnt = 0\n",
    "        for lay in s.layers:\n",
    "            data = lay.forward(data, to_print)\n",
    "            print(cnt, ' ', data)\n",
    "            cnt+=1\n",
    "        s.last_result = data\n",
    "        return s.last_result\n",
    "\n",
    "    def forward_nu(s, data):\n",
    "        for lay in s.layers:\n",
    "            data = lay.forward_nu(data)\n",
    "        last_result = data\n",
    "        return last_result\n",
    "        \n",
    "\n",
    "    \n",
    "    def backprop(s, correct):\n",
    "        err = s.last_result - correct\n",
    "        cnt = 0\n",
    "        for lay in reversed(s.layers[1:]):\n",
    "            #print(cnt)\n",
    "            err = lay.backprop(err)\n",
    "            cnt+=1\n",
    "\n",
    "    def backprop_error(s, err):\n",
    "        for lay in reversed(s.layers[1:]):\n",
    "            err = lay.backprop(err)\n",
    "\n",
    "    def update_w(s):\n",
    "        i = 0\n",
    "        for lay in reversed(s.layers[1:]):\n",
    "            #print('layer idx: ', i)\n",
    "            i+=1\n",
    "            lay.update_weights()\n",
    "            #print('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    def learn_one(s, in_data, target_data):\n",
    "        s.forward(in_data)\n",
    "        s.backprop(target_data)\n",
    "        s.update_w()   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def append_one(s, new_layer, check = False):\n",
    "        if check and s.n_of_layers!=0:\n",
    "            last_layer_size = s.layers[-1].size\n",
    "            if last_layer_size != new_layer.prev_size:\n",
    "                print(\"size not match, layer \", s.n_of_layers)\n",
    "                return\n",
    "        s.layers.append(new_layer)\n",
    "        s.n_of_layers+=1\n",
    "        return\n",
    "\n",
    "    def check_layers_sizes(s, check_layers):\n",
    "        for i in range(1, len(check_layers)):\n",
    "            if (check_layers[i-1].size != check_layers[i].prev_size):\n",
    "                print(\"error between \", i-1, \"and \", i)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def append_layers(s, new_layers):\n",
    "        test_layers = np.array([])\n",
    "        if s.n_of_layers != 0: # if has layers\n",
    "            test_layers = s.layers[-1] # get last layers\n",
    "        \n",
    "        test_layers = np.append(test_layers, new_layers) \n",
    "                \n",
    "        if (s.check_layers_sizes(test_layers)):\n",
    "            for lay in new_layers:\n",
    "                s.append_one(lay)\n",
    "            print(\"added LAYERS succesfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"ERROR adding layers, check info above\")\n",
    "            return False\n",
    "\n",
    "    def add_layer_by_pos(s, pos, new_layer):\n",
    "        if (pos <= 0 or pos > s.n_of_layers): # if input or more than 'to last'\n",
    "            print(\"ERROR addning layer: invalid layer number!\")\n",
    "            if (pos == 0):\n",
    "                print(\"input layer cannot be replaced by different layer\")\n",
    "            return\n",
    "            \n",
    "        if (pos == s.n_of_layers): # if add to the last\n",
    "            s.append(new_layer)\n",
    "            return\n",
    "            \n",
    "        if (new_layer.prev_size!=s.layers[pos-1].size):\n",
    "            print(\"ERROR addning layer: invalid prev_size!\")\n",
    "            return \n",
    "            \n",
    "        s.layers.insert(pos, new_layer)\n",
    "        prev_size = new_layer.size\n",
    "        \n",
    "        # update next layer prev_size and w matrix\n",
    "        next_lay = s.layers[pos+1]\n",
    "        next_lay.correct_prev_size(prev_size)\n",
    "        s.n_of_layers += 1\n",
    "\n",
    "    def delete_layer_by_pos(s, pos):\n",
    "        if (pos <= 0 or pos >= s.n_of_layers): # if input or more than 'to last'\n",
    "            print(\"ERROR deleting layer: invalid layer number!\")\n",
    "            if (pos == 0):\n",
    "                print(\"input layer cannot be deleted\")\n",
    "            return\n",
    "        \n",
    "        new_prev_size = s.layers[pos].prev_size \n",
    "        if (pos != s.n_of_layers-1): #if not last\n",
    "            next_lay = s.layers[pos+1]\n",
    "            next_lay.correct_prev_size(new_prev_size)\n",
    "\n",
    "        del s.layers[pos]\n",
    "        s.n_of_layers -= 1\n",
    "\n",
    "\n",
    "    def add_neuron(s, layer_number, n_of_neurons = 1):\n",
    "        if (layer_number < 0 or layer_number>= s.n_of_layers):\n",
    "            print(\"ERROR addning neuron: invalid layer number!\")\n",
    "            return\n",
    "        \n",
    "        main_lay = s.layers[layer_number]  \n",
    "        \n",
    "        for i in range(n_of_neurons):\n",
    "            main_lay.add_neuron()           \n",
    "            if (layer_number+1 != s.n_of_layers): # if main is not last\n",
    "                # update next layer prev_size and w matrix\n",
    "                next_lay = s.layers[layer_number+1]\n",
    "                next_lay.add_new_prev_size()\n",
    "    \n",
    "    def delete_neuron(s, layer_number, neuron_number):\n",
    "        if (layer_number < 0 or layer_number>= s.n_of_layers):\n",
    "            print(\"ERROR deleting neuron: invalid layer number!\")\n",
    "            return\n",
    "            \n",
    "        main_lay = s.layers[layer_number] \n",
    "        \n",
    "        if (neuron_number >= main_lay.size):\n",
    "            print(\"ERROR deleting neuron: invalid neuron number!\")\n",
    "            return\n",
    "\n",
    "        main_lay.delete_neuron(neuron_number)\n",
    "        if (layer_number+1 != s.n_of_layers): # if main is not last\n",
    "                # update next layer prev_size and w matrix\n",
    "                next_lay = s.layers[layer_number+1]\n",
    "                next_lay.delete_new_prev_size()\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def print_info(s):\n",
    "        print('NAME: ', s.name, ' (', s.n_of_layers, ')')\n",
    "        for cnt in range(s.n_of_layers):\n",
    "            print(\"#\", cnt)\n",
    "            s.layers[cnt].print_info()\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_pic(s):\n",
    "        print('NAME: ', s.name, ' (', s.n_of_layers, ')')\n",
    "        cnt = 0\n",
    "        for lay in s.layers:\n",
    "            lay.print_pic()\n",
    "            print(\"#\", cnt, end='')\n",
    "            cnt+=1\n",
    "        print(\"\\nOUT |#|\\nOUT  v\")\n",
    "\n",
    "    \n",
    "    def save(s, file_path):       \n",
    "        f = open(file_path, \"w\").close()\n",
    "        \n",
    "        f = open(file_path, \"a\")       \n",
    "        f.write(\"{}\\n{}\\n\".format(s.name, s.n_of_layers))       \n",
    "        input_layer_size = s.layers[0].get_info()\n",
    "        f.write(\"{}\\n\".format(input_layer_size))\n",
    "\n",
    "        for lay in s.layers[1:]:            \n",
    "            prev_size, size, weights, bias, activation_type, lr, optimizer, beta = lay.get_info()\n",
    "            f.write(\"{}\\n\".format(prev_size))\n",
    "            f.write(\"{}\\n\".format(size))\n",
    "            \n",
    "            np.savetxt(f, weights)#, fmt='%f')\n",
    "            np.savetxt(f, bias)#, fmt='%f')\n",
    "            f.write(\"{}\\n\".format(activation_type))\n",
    "            f.write(\"{}\\n\".format(lr))\n",
    "            f.write(\"{}\\n\".format(optimizer))\n",
    "            f.write(\"{}\\n\".format(beta))\n",
    "                 \n",
    "        f.close()\n",
    "\n",
    "        \n",
    "    def load(s, file_path):\n",
    "        s.layers = None\n",
    "        s.layers = []\n",
    "        s.n_of_layers = 0       \n",
    "        layers = []\n",
    "        f = open(file_path, \"r\")       \n",
    "        name = f.readline().split()[0]\n",
    "        total_n_of_layers = int(f.readline().split()[0])\n",
    "        input_layer_size = int(f.readline().split()[0])\n",
    "\n",
    "        in_layer = input_layer(input_layer_size)\n",
    "        layers.append(in_layer)\n",
    "        \n",
    "        s.give_name(name)\n",
    "\n",
    "        for i in range(total_n_of_layers-1):\n",
    "            prev_size = int(f.readline().split()[0])\n",
    "            size = int(f.readline().split()[0])\n",
    "            weights = np.loadtxt(f, max_rows = prev_size)\n",
    "            bias = np.loadtxt(f, max_rows = size)\n",
    "            activation_type = f.readline().split()[0]\n",
    "            lr = float(f.readline().split()[0])\n",
    "            optimizer = f.readline().split()[0]\n",
    "            beta = float(f.readline().split()[0])\n",
    "            \n",
    "            layers.append(layer(lr = lr, prev_size= prev_size, my_size = size, \n",
    "                                activation_type = activation_type, weights = weights, bias = bias, optimizer = optimizer, beta = beta))\n",
    "            \n",
    "        s.append(layers)             \n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562a6b3-fda4-40e2-95bc-742429af506f",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1177b3e4-b520-40f5-a7ee-a3fa39086d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61859f1a-70dc-4a12-bdbe-b53309e16ff6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d3bc73-6811-4ce3-8509-593bf9b018a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "train_size = 500\n",
    "data_x_offset = 20\n",
    "data_y_offset = 20\n",
    "in_data = [] \n",
    "target_data = []\n",
    "for i in range(train_size):\n",
    "    theta = np.random.random() * 2 * math.pi\n",
    "    radius = np.random.rand() * 0.3\n",
    "    if (i < train_size/4):\n",
    "        in_data.append([0 + radius*math.cos(theta), 0 + radius*math.sin(theta) ])\n",
    "        target_data.append([0])\n",
    "    elif (i < 2*train_size/4):\n",
    "        in_data.append([1 + radius*math.cos(theta), 1 + radius*math.sin(theta) ])\n",
    "        target_data.append([0])\n",
    "    elif(i < 3*train_size/4):\n",
    "        in_data.append([ 1 + radius*math.cos(theta), 0 + radius*math.sin(theta) ])\n",
    "        target_data.append([1])\n",
    "    else:\n",
    "        in_data.append([ 0 + radius*math.cos(theta), 1 + radius*math.sin(theta) ])\n",
    "        target_data.append([1])\n",
    "\n",
    "test_size = round(train_size*0.2)\n",
    "in_test_data = [] \n",
    "target_test_data = []\n",
    "for i in range(test_size):\n",
    "    theta = np.random.random() * 2 * math.pi\n",
    "    radius = np.random.rand() * 0.3\n",
    "    if (i < test_size/4):\n",
    "        in_test_data.append([0 + radius*math.cos(theta), 0 + radius*math.sin(theta) ])\n",
    "        target_test_data.append([0])\n",
    "    elif (i < 2*test_size/4):\n",
    "        in_test_data.append([1 + radius*math.cos(theta), 1 + radius*math.sin(theta) ])\n",
    "        target_test_data.append([0])\n",
    "    elif(i < 3*test_size/4):\n",
    "        in_test_data.append([ 1 + radius*math.cos(theta), 0 + radius*math.sin(theta) ])\n",
    "        target_test_data.append([1])\n",
    "    else:\n",
    "        in_test_data.append([ 0 + radius*math.cos(theta), 1 + radius*math.sin(theta) ])\n",
    "        target_test_data.append([1])\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(np.array(in_data)[:, 0], np.array(in_data)[:, 1], c = target_data)\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.title(\"training data\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(np.array(in_test_data)[:, 0], np.array(in_test_data)[:, 1], c = target_test_data)\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.title(\"validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64c49c-0b75-4e57-80c1-d3bab2bb037c",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c27cd8b-b6fa-48db-9230-f68a5f82c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added LAYERS succesfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "in_layer = input_layer(2)\n",
    "tl = layer(lr = 0.01, prev_size = 2, my_size=2, activation_type=\"ReLU\", optimizer = 'SGDwM', beta = 0.8)\n",
    "out_layer = layer(lr = 0.01, prev_size = 2, my_size=1, activation_type=\"Sigmoid\", optimizer = 'SGDwM', beta = 0.6)\n",
    "\n",
    "test_nn = plastic_nn()\n",
    "layers = [in_layer, tl, out_layer]\n",
    "\n",
    "test_nn.append_layers(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33025f62-53ca-4ddd-8ec2-796cd3a9f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  * ------------------------------------------------- ]\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m     display_bar(bar_len, i, epochs)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(in_data)):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#print(\"idx \", idx)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mtest_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m error_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(in_test_data)):\n",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m, in \u001b[0;36mplastic_nn.learn_one\u001b[1;34m(s, in_data, target_data)\u001b[0m\n\u001b[0;32m     65\u001b[0m s\u001b[38;5;241m.\u001b[39mforward(in_data)\n\u001b[0;32m     66\u001b[0m s\u001b[38;5;241m.\u001b[39mbackprop(target_data)\n\u001b[1;32m---> 67\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 59\u001b[0m, in \u001b[0;36mplastic_nn.update_w\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lay \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(s\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m#print('layer idx: ', i)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mlay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 84\u001b[0m, in \u001b[0;36mlayer.update_weights\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     81\u001b[0m s\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39minput_t \u001b[38;5;66;03m#.reshape((s.input_t.shape[0], 1))\u001b[39;00m\n\u001b[0;32m     82\u001b[0m s\u001b[38;5;241m.\u001b[39md \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;66;03m#.reshape((1, s.delta.shape[0]))\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m dw \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m db \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdelta\n\u001b[0;32m     87\u001b[0m s\u001b[38;5;241m.\u001b[39mVdw \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m*\u001b[39ms\u001b[38;5;241m.\u001b[39mVdw \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39ms\u001b[38;5;241m.\u001b[39mbeta)\u001b[38;5;241m*\u001b[39mdw\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "to_display_bar = True\n",
    "\n",
    "graph_err = []\n",
    "graph_epoch=[]\n",
    "epochs = 400\n",
    "target_data = np.array(target_data)\n",
    "#print(target_data)\n",
    "\n",
    "bar_len = 50\n",
    "\n",
    "def display_bar(bar_len, idx, total):\n",
    "    idx = int(idx*bar_len/total)\n",
    "    #print(perc)\n",
    "    print(\"[\", \"-\"*idx, \"*\", \"-\"*(bar_len-idx-1), \"]\", end='')\n",
    "    print(\"\\r\",end='')\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    if to_display_bar:\n",
    "        display_bar(bar_len, i, epochs)\n",
    "    \n",
    "    for idx in range(len(in_data)):\n",
    "        #print(\"idx \", idx)\n",
    "        test_nn.learn_one(in_data[idx], target_data[idx])\n",
    "\n",
    "\n",
    "\n",
    "    error_sum = 0\n",
    "    for idx in range(len(in_test_data)):\n",
    "        valid_res = test_nn.forward(in_test_data[idx])\n",
    "        #print(target_test_data[idx])\n",
    "        #print(valid_res)\n",
    "        error = (target_test_data[idx] - valid_res)**2\n",
    "        error_sum+=error\n",
    "    \n",
    "    graph_err.append(error_sum/len(in_test_data))\n",
    "    graph_epoch.append(i)\n",
    "\n",
    "\n",
    "\n",
    "#print(\"graph_epoch \", graph_epoch)\n",
    "#print(\"(graph_err)\", graph_err)\n",
    "plt.figure()\n",
    "plt.scatter(graph_epoch, graph_err)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf559e-388f-424d-adde-f607f2c9b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2_in = [[0, 0], [0,1], [1, 0], [1, 1],\n",
    "            [0.2, 0.1], [-0.1, 1], [1.1, -0.1], [1.2, 1.1]]\n",
    "for i in range(len(test_2_in)):\n",
    "    print(test_nn.forward(test_2_in[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
