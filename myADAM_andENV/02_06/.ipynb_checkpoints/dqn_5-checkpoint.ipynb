{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "K5feZBXDqSyO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5feZBXDqSyO",
    "outputId": "f8fa2bd4-7df7-40c6-97a6-3a71739035a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290e9d45-e28b-4603-9058-3260849a1668",
   "metadata": {
    "id": "290e9d45-e28b-4603-9058-3260849a1668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from TrulyPlastic_allOpt_5.ipynb\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "from TrulyPlastic_allOpt_5 import plastic_nn\n",
    "from TrulyPlastic_allOpt_5 import input_layer\n",
    "from TrulyPlastic_allOpt_5 import layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cea6a06-3557-4e61-a769-420d9de43ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class replay_memory():\n",
    "    def __init__(s, maxlen):\n",
    "        s.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(s, transition):\n",
    "        s.memory.append(transition)\n",
    "\n",
    "    def sample(s, sample_size):\n",
    "        return random.sample(s.memory, sample_size)\n",
    "\n",
    "    def __len__(s):\n",
    "        return len(s.memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN():\n",
    "   \n",
    "    def __init__(s, ct = 0, tag=0, path = r'test', \n",
    "                 game_name = 'MountainCar-v0', \n",
    "                  \n",
    "                 mini_batch_size = 32,  num_divisions = 1, \n",
    "                 replay_memory_size = 100000, \n",
    "                 network_sync_rate = 50000, discount_factor_g = 0.9):\n",
    "        \n",
    "        if (ct == 0):\n",
    "            ct = datetime.datetime.now()\n",
    "            ct = str(ct)\n",
    "            ct = ct.replace(\":\", \"-\")\n",
    "            ct = ct.replace(\" \", \"_\")\n",
    "            ct = ct[:-7]\n",
    "\n",
    "        s.ct = ct\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory_pics = os.path.join(current_directory, s.ct)\n",
    "        \n",
    "        final_directory_pics = os.path.join(final_directory_pics, 'pics')\n",
    "        if not os.path.exists(final_directory_pics):\n",
    "            os.makedirs(final_directory_pics)\n",
    "        \n",
    "        if (tag != 0):\n",
    "            s.set_tag(tag)\n",
    "\n",
    "        s.game_name = game_name\n",
    "        s.discount_factor_g = discount_factor_g\n",
    "         \n",
    "        s.mini_batch_size = mini_batch_size \n",
    "        s.num_divisions = num_divisions\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        s.lin_spaces = []\n",
    "        env = gym.make(s.game_name)\n",
    "        obs_space = env.observation_space\n",
    "        \n",
    "        for i in range(obs_space.shape[0]):\n",
    "            s.lin_spaces.append(np.linspace(env.observation_space.low[i], env.observation_space.high[i], s.num_divisions))\n",
    "\n",
    "        env.close()\n",
    "        \n",
    "        \n",
    "        s.replay_memory_size =  replay_memory_size \n",
    "        s.network_sync_rate = network_sync_rate\n",
    "        \n",
    "    def set_tag(s, tag):\n",
    "        s.tag = tag\n",
    "        s.path = s.ct+'/'+s.tag\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory = os.path.join(current_directory, s.path)\n",
    "        if not os.path.exists(final_directory):\n",
    "            os.makedirs(final_directory)\n",
    "        \n",
    "   \n",
    "    def plot_progress(self, rewards_per_episode_, epsilon_history_):\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('reward')\n",
    "        plt.plot(rewards_per_episode_)\n",
    "\n",
    "        plt.savefig(f'{self.path}/info_rew_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.savefig(f'{self.ct}/pics/info_rew_{self.tag}.png')\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('epsilon')\n",
    "        plt.plot(epsilon_history_)\n",
    "        plt.savefig(f'{self.path}/info_eps_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, policy_dqn, episodes, render=False):\n",
    "        target_dqn = plastic_nn()\n",
    "        target_dqn = policy_dqn.deep_copy()\n",
    "\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "\n",
    "        \n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = replay_memory(self.replay_memory_size)\n",
    "\n",
    "        rewards_per_episode = []\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count = 0\n",
    "        goal_reached = False\n",
    "        best_rewards = -200\n",
    "\n",
    "        for i in range(episodes+1):\n",
    "            state = env.reset()[0]  # Initialize to state 0 seed=int(i+10)\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False\n",
    "            rewards = 0\n",
    "\n",
    "            while(not terminated and rewards < 300):\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    res = policy_dqn.forward(self.state_to_dqn_input(state))\n",
    "                    action = res.argmax().item()\n",
    "\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "                rewards += reward\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "                state = new_state\n",
    "                \n",
    "                step_count+=1\n",
    "\n",
    "\n",
    "            rewards_per_episode.append(rewards)\n",
    "            \n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                self.add_log_data(f'Episode {i} Epsilon {epsilon}')\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                self.add_log_data(f'Best rewards so far: {best_rewards}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_best'.format(self.path))\n",
    "                \n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size and goal_reached:\n",
    "                \n",
    "                #print(f'OPTIMIZE Episode {i} Epsilon {epsilon} rewards {rewards}') # print(rewards)\n",
    "\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = epsilon = max(epsilon - 1/episodes, 0.01) # max(epsilon*0.99996, 0.05)#\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn = policy_dqn.deep_copy()\n",
    "                    step_count = 0\n",
    "                   \n",
    "                \n",
    "                \n",
    "        env.close()\n",
    "        policy_dqn.save(f'{self.path}/mc_policy_last_{self.tag}'.format(self.path, self.tag))\n",
    "        if (best_rewards == -200):\n",
    "            policy_dqn.save(f'{self.path}/mc_policy_best'.format(self.path))\n",
    "        self.save_reward_data(rewards_per_episode)\n",
    "        #print(rewards_per_episode)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        target_q_list = []\n",
    "        input_list = []\n",
    "        \n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor_g * target_dqn.forward(self.state_to_dqn_input(new_state)).max()\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            state_dsc = np.asarray(self.state_to_dqn_input(state))\n",
    "            input_list.append(state_dsc)\n",
    "            \n",
    "            target_q = target_dqn.forward(state_dsc)\n",
    "            \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target            \n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        #BACKPOP AND UPDATE on minibatch\n",
    "        x = np.asarray(input_list)\n",
    "        x = x[:, :, 0]\n",
    "        x = x.T\n",
    "\n",
    "        y = np.asarray(target_q_list)\n",
    "        y = y[:, :, 0]\n",
    "        y = y.T\n",
    "\n",
    "        policy_dqn.learn_one(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_dqn_input(s, state):\n",
    "        # d_state = []\n",
    "        # for i in range(state.shape[0]):\n",
    "        #     dig = np.digitize(state[i], s.lin_spaces[i])\n",
    "        #     d_state.append(np.asarray([dig]))\n",
    "\n",
    "        # return np.asarray(d_state)\n",
    "        return np.asarray([[state[0]], [state[1]], [state[2]], [state[3]]])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def test(self, policy_dqn, episodes, render = False):\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "        \n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        done_count = 0\n",
    "        medium_reward = 0\n",
    "        reward_list = []\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            state, info = env.reset() \n",
    "            done = False\n",
    "            truncated = False \n",
    "            rewards = 0\n",
    "\n",
    "            while(not done and not truncated):\n",
    "                state = self.state_to_dqn_input(state)\n",
    "\n",
    "                res = policy_dqn.forward(state)\n",
    "\n",
    "                action = res.argmax().item()\n",
    "\n",
    "                state, reward, done, truncated, _ = env.step(action)\n",
    "                rewards+=reward\n",
    "                if (truncated):\n",
    "                    done_count += 1\n",
    "                    break\n",
    "\n",
    "            medium_reward += rewards\n",
    "            reward_list.append(rewards)\n",
    "                \n",
    "\n",
    "        \n",
    "        env.close()\n",
    "        medium_reward = medium_reward / episodes\n",
    "        return done_count*100.0/episodes, medium_reward, reward_list\n",
    "        \n",
    "    def save_info(s, info):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\")   \n",
    "       \n",
    "\n",
    "\n",
    "        f.write(\"data {}\\n\".format(s.ct))\n",
    "        f.write(\"tag {}\\n\".format(s.tag))\n",
    "        f.write(\"game_name {}\\n\".format(s.game_name))\n",
    "        f.write(\"reward discount factor {}\\n\".format(s.discount_factor_g))\n",
    "        f.write(\"minibatch size {}\\n\".format(s.mini_batch_size))\n",
    "        f.write(\"num divisions{}\\n\".format(s.num_divisions))\n",
    "        f.write(\"replay memory size {}\\n\".format(s.replay_memory_size))\n",
    "        f.write(\"network sync rate {}\\n\".format(s.network_sync_rate))\n",
    "        f.write(\"info {}\\n\".format(info))\n",
    "                         \n",
    "        f.close()\n",
    "    \n",
    "    def add_log_data(s, data):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n",
    "        \n",
    "    def save_reward_data(s, data):\n",
    "        file_path = f'{s.path}/rewards_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb6427-5c7b-4b3c-a146-0c79966845bc",
   "metadata": {},
   "source": [
    "## alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aacb33b8-094a-4c17-8f70-cc7e2b4a89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_search():\n",
    "    def __init__(s, dqn_model = 0, policy_dqn = 0):\n",
    "        if (policy_dqn == 0):\n",
    "            s.policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "        else:\n",
    "            s.policy_dqn = policy_dqn\n",
    "\n",
    "        if (dqn_model == 0):\n",
    "            s.dqn_model = DQN()\n",
    "        else:\n",
    "            s.dqn_model = dqn_model\n",
    "        pass\n",
    "\n",
    "    def set_DQN(s, dqn_model):\n",
    "        s.dqn_model = dqn_model\n",
    "        \n",
    "    def set_NN(s, policy_dqn):\n",
    "        s.policy_dqn = policy_dqn\n",
    "\n",
    "    def set_a_type_array(s, a_type):\n",
    "        s.atype = a_type\n",
    "\n",
    "    def set_lr(s, lr):\n",
    "        s.lr = lr\n",
    "\n",
    "    def set_nn_topology(s, layers_net):\n",
    "        s.policy_dqn.delete_layers()\n",
    "        s.policy_dqn.append_layers(layers_net)\n",
    "        s.policy_dqn.optimizer_reset()\n",
    "                \n",
    "    \n",
    "\n",
    "    def create_net(s, shape):\n",
    "        layers_net = []\n",
    "        layers_net.append(input_layer(shape[0]))\n",
    "        for i in range(1, len(shape)):\n",
    "            new_layer = layer(lr = s.lr, prev_size = shape[i-1], my_size=shape[i], activation_type=s.atype[i])\n",
    "            # new_layer.print_info()\n",
    "            layers_net.append(new_layer)\n",
    "        return layers_net\n",
    "\n",
    "    def train(s, epochs):       \n",
    "        s.dqn_model.train(s.policy_dqn, epochs, False)\n",
    "\n",
    "    def update_info_tag(s, m, shape):\n",
    "\n",
    "        shape_tag= ''.join(str(x) for x in shape)\n",
    "        s.dqn_model.set_tag(tag = shape_tag+'_'+str(m))\n",
    "        s.dqn_model.save_info(f'lr: {s.lr} \\nshape:{shape} \\na1:{s.atype}\\n')\n",
    "\n",
    "    def test(s, n_tests_for_model):\n",
    "        path = s.dqn_model.path\n",
    "\n",
    "        s.policy_dqn.load(f\"{path}/mc_policy_best\")\n",
    "\n",
    "        return s.dqn_model.test(s.policy_dqn, n_tests_for_model, render = False)\n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "    def search(s, n_models, TSR, start_shape, shape_steps, final_shape,\n",
    "              epochs = 2000, n_tests_for_model = 100,\n",
    "               adaptive_n_models = False, min_best_result = 0\n",
    "              ):\n",
    "        \n",
    "        shape = start_shape\n",
    "\n",
    "        steps_dif = shape_steps.shape[0]\n",
    "        step_i = 0\n",
    "\n",
    "        flag = False\n",
    "        adapt_cnt = 0\n",
    "        while not (np.array_equal(shape, final_shape)):  \n",
    "            m = 0\n",
    "            best_result = min_best_result\n",
    "            while (m < n_models): \n",
    "                print(m, shape)\n",
    "                s.set_nn_topology(s.create_net(shape))\n",
    "                if (flag):\n",
    "                    numb = str(m) +'_'+str(adapt_cnt)\n",
    "                else:\n",
    "                    numb = m\n",
    "                s.update_info_tag(numb, shape)\n",
    "\n",
    "                #train\n",
    "                s.train(epochs)\n",
    "\n",
    "                # test model\n",
    "                test_res, medium_reward, reward_list = s.test(n_tests_for_model)\n",
    "                \n",
    "                print('test_res ', test_res)\n",
    "                print('medium_reward ', medium_reward)\n",
    "\n",
    "                if (test_res >= TSR):\n",
    "                    print('success shape: ', shape)\n",
    "                    return test_res, shape, s.policy_dqn\n",
    "\n",
    "                if (adaptive_n_models):\n",
    "                    if (test_res > best_result):\n",
    "                        m -= 1\n",
    "                        if (flag == True):\n",
    "                            adapt_cnt+=1\n",
    "                        flag = True\n",
    "                        best_result = test_res\n",
    "                    else:\n",
    "                        flag = False\n",
    "                        adapt_cnt = 0\n",
    "                m+=1\n",
    "            \n",
    "            #shape step\n",
    "            step_mask = shape_steps[step_i]\n",
    "            step_i += 1\n",
    "            step_i %= steps_dif\n",
    "    \n",
    "            shape+=step_mask\n",
    "\n",
    "    \n",
    "        print('search falied') \n",
    "        return False, 0, 0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
