{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa1c7b5-8a91-4a55-80f1-3e92beef3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad536855-8dd3-4cad-8ddb-c4fdbac5bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    res = 1 / (1 + np.exp(-x))\n",
    "    return res\n",
    "\n",
    "def d_Sigmoid(x):\n",
    "    y = Sigmoid(x) * (1 - Sigmoid(x))\n",
    "    return y\n",
    "\n",
    "def ReLU(x):\n",
    "    x = np.maximum(0, x)\n",
    "    return x\n",
    "    \n",
    "def d_ReLU(x):\n",
    "    y=x.copy()\n",
    "    y[y<=0] = 0\n",
    "    y[y>0] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def Linear(x):\n",
    "    return x\n",
    "\n",
    "def d_Linear(x):\n",
    "    y = np.ones(shape=(x.shape), dtype = x.dtype)\n",
    "    return y\n",
    "\n",
    "activations_dict = {\n",
    "'Sigmoid': [Sigmoid, d_Sigmoid],\n",
    "'ReLU': [ReLU, d_ReLU], \n",
    "'Linear': [Linear, d_Linear]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c9571-2702-416f-b3f6-3a202873fb3a",
   "metadata": {},
   "source": [
    "## input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11960885-56e4-42ee-9271-3141c226f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class input_layer:\n",
    "    def __init__(s, size):\n",
    "        s.size = size\n",
    "        #s.values = np.zeros(shape=(size), dtype = float)\n",
    "    \n",
    "    def add_neuron(s):\n",
    "        #add_v = np.zeros(shape=(n_of_neurons), dtype=float)\n",
    "        #s.values = np.concatenate((s.values, add_v.T))\n",
    "        s.size += 1\n",
    "\n",
    "    def delete_neuron(s, neuron_number):\n",
    "        s.size-=1\n",
    "        \n",
    "    def delete_new_prev_size(s):     \n",
    "        return\n",
    "\n",
    "\n",
    "    def add_new_prev_size(s):     \n",
    "        return\n",
    "\n",
    "        \n",
    "    def print_info(s):\n",
    "        print(\"IN LAYER\\nsize: \", s.size)\n",
    "\n",
    "    \n",
    "    def print_pic(s):\n",
    "        print_size = min(2, s.size)\n",
    "\n",
    "        for i in range(print_size): \n",
    "            print(\"| |\\t\", end='')\n",
    "        print(\"\")\n",
    "        for i in range(print_size):\n",
    "            print(\" v \\t\", end='')\n",
    "        print(\"\")\n",
    "        for i in range(print_size):\n",
    "            print(' @\\t', end='')\n",
    "        print (\"--\", format(s.size, ' 5d') , \"--\\t\", end='')\n",
    "\n",
    "    def forward(s, x, to_print = False):\n",
    "        s.values = x\n",
    "        return x\n",
    "        \n",
    "    def forward_nu(s, x):\n",
    "        return x\n",
    "\n",
    "    def get_info(s):\n",
    "        return s.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0c956-c5d1-4c97-a5c1-f9834d29b162",
   "metadata": {},
   "source": [
    "## layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42278612-f311-452d-93eb-c6ce6785a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(s, lr = 0.1, prev_size = 2, my_size=2, activation_type = \"Sigmoid\", weights = None, bias = None):\n",
    "        s.lr = lr\n",
    "        s.size = my_size\n",
    "        s.prev_size = prev_size\n",
    "        if (np.all(weights == None)):\n",
    "            # s.weights = np.random.random((prev_size, s.size))\n",
    "            s.w = np.random.random((s.size, prev_size))\n",
    "        else:\n",
    "            s.w = weights.copy()\n",
    "            s.w = s.w.reshape((s.size, prev_size))\n",
    "            \n",
    "        if (np.all(bias == None)):\n",
    "            s.b = np.random.random((s.size, 1))\n",
    "        else:\n",
    "            s.b = bias.copy()\n",
    "            s.b = s.b.reshape((s.size, 1))\n",
    "            \n",
    "        s.activation_type = activation_type\n",
    "        funcs = activations_dict.get(activation_type)\n",
    "        s.activation_f = funcs[0]\n",
    "        s.d_activation_f = funcs[1]\n",
    "        \n",
    "        s.optimizer_reset()\n",
    "        s.epsilon = 1e-8\n",
    "\n",
    "    def optimizer_reset(s):\n",
    "        s.Vdw = np.zeros(shape=(s.size, s.prev_size))\n",
    "        s.Vdb = np.zeros(shape=(s.size, 1))\n",
    "        \n",
    "        s.Sdw = np.zeros(shape=(s.size, s.prev_size))\n",
    "        s.Sdb = np.zeros(shape=(s.size, 1))\n",
    "        s.t = 1\n",
    "\n",
    "    def activate(s, x):\n",
    "        return s.activation_f(x)\n",
    "        \n",
    "    def d_activate(s, x):\n",
    "        return s.d_activation_f(x)  \n",
    "\n",
    "    \n",
    "    def forward(s, x, to_print = False):\n",
    "        s.x = np.asarray(x)\n",
    "        s.z = np.dot(s.w, s.x) + s.b\n",
    "        \n",
    "        if (to_print): \n",
    "            print('wT * x + b', s.z)\n",
    "\n",
    "        s.a = s.activate(s.z)\n",
    "\n",
    "        if (to_print): \n",
    "            print('s.a ',s.a)\n",
    "            \n",
    "        return s.a\n",
    "\n",
    "    def backprop(s, da):\n",
    "        s.dz = da * s.d_activate(s.z)\n",
    "        s.da_ = np.dot(s.w.T, s.dz) \n",
    "\n",
    "        \n",
    "        return s.da_\n",
    "        \n",
    "    def update_weights(s, optimizer = \"SGD\", beta1 = 0.9, beta2 = 0.999):\n",
    "        \n",
    "        m = s.x.shape[1]\n",
    "        if (optimizer == \"SGD\"):\n",
    "            \n",
    "            s.dw = (1/m)*np.dot(s.dz, s.x.T)\n",
    "            s.db = (1/m)*np.sum(s.dz, axis = 1, keepdims = True)\n",
    "    \n",
    "            s.w = s.w - s.lr * s.dw\n",
    "            s.b = s.b - s.lr * s.db\n",
    "            \n",
    "        elif (optimizer==\"SGDwM\"):\n",
    "\n",
    "            s.dw = (1/m)*np.dot(s.dz, s.x.T)\n",
    "            s.db = (1/m)*np.sum(s.dz, axis = 1, keepdims = True)\n",
    "\n",
    "            \n",
    "            s.Vdw = beta1 * s.Vdw + (1 - beta1)*s.dw\n",
    "            s.Vdb = beta1 * s.Vdb + (1 - beta1)*s.db\n",
    "            \n",
    "            s.w = s.w - s.lr * s.Vdw\n",
    "            s.b = s.b - s.lr * s.Vdb\n",
    "            \n",
    "        elif (optimizer==\"RMSProp\"):\n",
    "\n",
    "            s.dw = (1/m)*np.dot(s.dz, s.x.T)\n",
    "            s.db = (1/m)*np.sum(s.dz, axis = 1, keepdims = True)\n",
    "\n",
    "            s.Sdw = beta2 * s.Sdw + (1-beta2) * np.square(s.dw)\n",
    "            s.Sdb = beta2 * s.Sdb + (1-beta2) * np.square(s.db)\n",
    "            \n",
    "            s.w = s.w - s.lr * s.dw / (np.sqrt(s.Sdw))\n",
    "            s.b = s.b - s.lr * s.db / (np.sqrt(s.Sdb))\n",
    "            \n",
    "        elif (optimizer==\"Adam\"):\n",
    "\n",
    "            s.dw = (1/m)*np.dot(s.dz, s.x.T)\n",
    "            s.db = (1/m)*np.sum(s.dz, axis = 1, keepdims = True)\n",
    "            \n",
    "            s.Vdw = beta1 * s.Vdw + (1 - beta1)*s.dw\n",
    "            s.Vdb = beta1 * s.Vdb + (1 - beta1)*s.db\n",
    "            \n",
    "            s.Sdw = beta2 * s.Sdw + (1-beta2) * np.square(s.dw)\n",
    "            s.Sdb = beta2 * s.Sdb + (1-beta2) * np.square(s.db)\n",
    "\n",
    "            # correct\n",
    "\n",
    "            s.Vdw_ = s.Vdw / (1 - beta1**s.t)\n",
    "            s.Vdb_ = s.Vdb / (1 - beta1**s.t)\n",
    "            s.Sdw_ = s.Sdw / (1 - beta2**s.t)\n",
    "            s.Sdb_ = s.Sdb / (1 - beta2**s.t)\n",
    "            \n",
    "            s.w = s.w - s.lr * s.Vdw_ / (np.sqrt(s.Sdw_) + s.epsilon)\n",
    "            s.b = s.b - s.lr * s.Vdb_ / (np.sqrt(s.Sdb_)  + s.epsilon)\n",
    "            s.t += 1\n",
    "        else:\n",
    "            print(\"NO SUCH OPTIMIZER!\")\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "            \n",
    "        \n",
    "    def print_info(s):\n",
    "        print(\"my size: \", s.size)\n",
    "        print(\"prev size: \", s.prev_size)\n",
    "        print(\"w: \", s.w.shape, s.w, \"\\n\")\n",
    "        print(\"b: \", s.b.shape, s.b, \"\\n\")\n",
    "\n",
    "              \n",
    "    def print_pic(s):\n",
    "        print_size = min(2, s.size)\n",
    "        print(\"\\nâ•»...\\nv...\")\n",
    "        for i in range(print_size):\n",
    "            print('O\\t', end='')\n",
    "        print (\"--\", format(s.size, ' 5d') , \"--\\t\", end='')\n",
    "\n",
    "\n",
    "\n",
    "    def correct_prev_size(s, new_prev_szie):\n",
    "        dif = new_prev_szie - s.prev_size\n",
    "        if dif > 0: # new prev is greater\n",
    "            for i in range(dif):\n",
    "                s.add_new_prev_size()\n",
    "        elif dif < 0:\n",
    "            dif*=-1\n",
    "            for i in range(dif):\n",
    "                s.delete_new_prev_size()\n",
    "        s.prev_size = new_prev_szie\n",
    "\n",
    "\n",
    "    def delete_neuron(s, neuron_number):\n",
    "        s.w = np.delete(s.w, neuron_number, axis = 0)\n",
    "        s.b = np.delete(s.b, neuron_number, axis = 0)\n",
    "        s.size-=1\n",
    "        \n",
    "    def delete_new_prev_size(s):     \n",
    "        s.w = np.delete(s.w, 0, axis = 1)\n",
    "        s.prev_size -=1\n",
    "\n",
    "\n",
    "    def add_neuron(s):     \n",
    "        add_w = np.zeros(shape=(1, s.prev_size), dtype=float) + 0.1 # np.random.random((s.prev_size, n_of_neurons)) #\n",
    "        s.w = np.concatenate((s.w, add_w), axis = 0)\n",
    "        add_b = np.zeros(shape=(1, 1), dtype=float) + 0.1 \n",
    "        s.b = np.concatenate((s.b, add_b))\n",
    "        s.size+=1\n",
    "\n",
    "    def add_new_prev_size(s):     \n",
    "        add_w = np.zeros(shape=(1, s.size), dtype=float) + 0.1\n",
    "        s.w = np.concatenate((s.w, add_w.T), axis = 1)\n",
    "        s.prev_size += 1\n",
    "\n",
    "    def get_info(s):\n",
    "        return s.prev_size, s.size, s.w, s.b, s.activation_type, s.lr\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae710b4e-f290-48ea-b91a-3cf97dd926f2",
   "metadata": {},
   "source": [
    "## plastic nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5958be-6ee5-4b92-9c34-7a695b81f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class plastic_nn:\n",
    "    def __init__(s, optimizer = \"SGD\", beta = 0.9):\n",
    "\n",
    "        opt_list = ['SGD', 'SGDwM', 'RMSProp', 'Adam']\n",
    "        if optimizer in opt_list:\n",
    "            s.optimizer = optimizer\n",
    "        else:\n",
    "            print('no such optimizer, available are: ')\n",
    "            for each in opt_list:\n",
    "                print(each)\n",
    "            return\n",
    "        \n",
    "        s.layers = []\n",
    "            \n",
    "        s.n_of_layers = 0\n",
    "        s.name = 'noname'\n",
    "        s.optimizer = optimizer\n",
    "        \n",
    "        s.beta = beta\n",
    "        pass\n",
    "\n",
    "    def give_name(s, name):\n",
    "        s.name = name\n",
    "        \n",
    "    def set_num_of_layers(s, num):\n",
    "        s.n_of_layers = num\n",
    "        \n",
    "    def deep_copy(s):\n",
    "        return copy.deepcopy(s)\n",
    "\n",
    "    \n",
    "    def forward(s, x, to_print = False):\n",
    "        for lay in s.layers:\n",
    "            x = lay.forward(x, to_print)\n",
    "        s.last_result = x\n",
    "        return s.last_result\n",
    "        \n",
    "    def forward_print(s, x, to_print = False):\n",
    "        print('in: ',data)\n",
    "        cnt = 0\n",
    "        for lay in s.layers:\n",
    "            x = lay.forward(x, to_print)\n",
    "            print(cnt, ' ', x)\n",
    "            cnt+=1\n",
    "        s.last_result = x\n",
    "        return s.last_result\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def backprop(s, correct):\n",
    "        m = correct.shape[1]\n",
    "        err = (s.last_result - correct) # a - y\n",
    "        cnt = 0\n",
    "        for lay in reversed(s.layers[1:]):\n",
    "            #print(cnt)\n",
    "            err = lay.backprop(err)\n",
    "            cnt+=1\n",
    "\n",
    "    def backprop_error(s, err):\n",
    "        for lay in reversed(s.layers[1:]):\n",
    "            err = lay.backprop(err)\n",
    "\n",
    "    def update(s):\n",
    "        i = 0\n",
    "        for lay in reversed(s.layers[1:]):\n",
    "            #print('layer idx: ', i)\n",
    "            i+=1\n",
    "            lay.update_weights(s.optimizer,s.beta)\n",
    "            #print('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    def learn_one(s, in_data, target_data):\n",
    "        s.forward(in_data)\n",
    "        s.backprop(target_data)\n",
    "        s.update()   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def append_one(s, new_layer, check = False):\n",
    "        if check and s.n_of_layers!=0:\n",
    "            last_layer_size = s.layers[-1].size\n",
    "            if last_layer_size != new_layer.prev_size:\n",
    "                print(\"size not match, layer \", s.n_of_layers)\n",
    "                return\n",
    "        s.layers.append(new_layer)\n",
    "        s.n_of_layers+=1\n",
    "        return\n",
    "\n",
    "    def check_layers_sizes(s, check_layers):\n",
    "        for i in range(1, len(check_layers)):\n",
    "            if (check_layers[i-1].size != check_layers[i].prev_size):\n",
    "                print(\"error between \", i-1, \"and \", i)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def append_layers(s, new_layers, to_print = False):\n",
    "        test_layers = np.array([])\n",
    "        if s.n_of_layers != 0: # if has layers\n",
    "            test_layers = s.layers[-1] # get last layers\n",
    "        \n",
    "        test_layers = np.append(test_layers, new_layers) \n",
    "                \n",
    "        if (s.check_layers_sizes(test_layers)):\n",
    "            for lay in new_layers:\n",
    "                s.append_one(lay)\n",
    "            if (to_print):\n",
    "                print(\"added LAYERS succesfully\")\n",
    "            return True\n",
    "        else:\n",
    "            if (to_print):\n",
    "                print(\"ERROR adding layers, check info above\")\n",
    "            return False\n",
    "\n",
    "    def add_layer_by_pos(s, pos, new_layer):\n",
    "        if (pos <= 0 or pos > s.n_of_layers): # if input or more than 'to last'\n",
    "            print(\"ERROR addning layer: invalid layer number!\")\n",
    "            if (pos == 0):\n",
    "                print(\"input layer cannot be replaced by different layer\")\n",
    "            return\n",
    "            \n",
    "        if (pos == s.n_of_layers): # if add to the last\n",
    "            s.append(new_layer)\n",
    "            return\n",
    "            \n",
    "        if (new_layer.prev_size!=s.layers[pos-1].size):\n",
    "            print(\"ERROR addning layer: invalid prev_size!\")\n",
    "            return \n",
    "            \n",
    "        s.layers.insert(pos, new_layer)\n",
    "        prev_size = new_layer.size\n",
    "        \n",
    "        # update next layer prev_size and w matrix\n",
    "        next_lay = s.layers[pos+1]\n",
    "        next_lay.correct_prev_size(prev_size)\n",
    "        s.n_of_layers += 1\n",
    "\n",
    "    def delete_layer_by_pos(s, pos):\n",
    "        if (pos <= 0 or pos >= s.n_of_layers): # if input or more than 'to last'\n",
    "            print(\"ERROR deleting layer: invalid layer number!\")\n",
    "            if (pos == 0):\n",
    "                print(\"input layer cannot be deleted\")\n",
    "            return\n",
    "        \n",
    "        new_prev_size = s.layers[pos].prev_size \n",
    "        if (pos != s.n_of_layers-1): #if not last\n",
    "            next_lay = s.layers[pos+1]\n",
    "            next_lay.correct_prev_size(new_prev_size)\n",
    "\n",
    "        del s.layers[pos]\n",
    "        s.n_of_layers -= 1\n",
    "\n",
    "\n",
    "    def add_neuron(s, layer_number, n_of_neurons = 1):\n",
    "        if (layer_number < 0 or layer_number>= s.n_of_layers):\n",
    "            print(\"ERROR addning neuron: invalid layer number!\")\n",
    "            return\n",
    "        \n",
    "        main_lay = s.layers[layer_number]  \n",
    "        \n",
    "        for i in range(n_of_neurons):\n",
    "            main_lay.add_neuron()           \n",
    "            if (layer_number+1 != s.n_of_layers): # if main is not last\n",
    "                # update next layer prev_size and w matrix\n",
    "                next_lay = s.layers[layer_number+1]\n",
    "                next_lay.add_new_prev_size()\n",
    "    \n",
    "    def delete_neuron(s, layer_number, neuron_number):\n",
    "        if (layer_number < 0 or layer_number>= s.n_of_layers):\n",
    "            print(\"ERROR deleting neuron: invalid layer number!\")\n",
    "            return\n",
    "            \n",
    "        main_lay = s.layers[layer_number] \n",
    "        \n",
    "        if (neuron_number >= main_lay.size):\n",
    "            print(\"ERROR deleting neuron: invalid neuron number!\")\n",
    "            return\n",
    "\n",
    "        main_lay.delete_neuron(neuron_number)\n",
    "        if (layer_number+1 != s.n_of_layers): # if main is not last\n",
    "                # update next layer prev_size and w matrix\n",
    "                next_lay = s.layers[layer_number+1]\n",
    "                next_lay.delete_new_prev_size()\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def print_info(s):\n",
    "        print('NAME: ', s.name, ' (', s.n_of_layers, ')')\n",
    "        for cnt in range(s.n_of_layers):\n",
    "            print(\"#\", cnt)\n",
    "            s.layers[cnt].print_info()\n",
    "            print(\"\")\n",
    "    \n",
    "    def print_pic(s):\n",
    "        print('NAME: ', s.name, ' (', s.n_of_layers, ')')\n",
    "        cnt = 0\n",
    "        for lay in s.layers:\n",
    "            lay.print_pic()\n",
    "            print(\"#\", cnt, end='')\n",
    "            cnt+=1\n",
    "        print(\"\\nOUT |#|\\nOUT  v\")\n",
    "\n",
    "    def optimizer_reset(s):\n",
    "        for lay in reversed(s.layers[1:]):\n",
    "            lay.optimizer_reset()\n",
    "\n",
    "    \n",
    "    def save(s, file_path):       \n",
    "        f = open(file_path, \"w\").close()\n",
    "        \n",
    "        f = open(file_path, \"a\")       \n",
    "        f.write(\"{}\\n{}\\n\".format(s.name, s.n_of_layers))       \n",
    "        input_layer_size = s.layers[0].get_info()\n",
    "        f.write(\"{}\\n\".format(input_layer_size))\n",
    "\n",
    "        for lay in s.layers[1:]:            \n",
    "            prev_size, size, weights, bias, activation_type, lr = lay.get_info()\n",
    "            f.write(\"{}\\n\".format(prev_size))\n",
    "            f.write(\"{}\\n\".format(size))\n",
    "            \n",
    "            np.savetxt(f, weights)#, fmt='%f')\n",
    "            np.savetxt(f, bias)#, fmt='%f')\n",
    "            f.write(\"{}\\n\".format(activation_type))\n",
    "            f.write(\"{}\\n\".format(lr))\n",
    "        \n",
    "        ct = datetime.datetime.now()\n",
    "\n",
    "\n",
    "        f.write(\"\\nct {}\\n\".format(ct))\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "    def load(s, file_path):\n",
    "        s.layers = None\n",
    "        s.layers = []\n",
    "        s.n_of_layers = 0       \n",
    "        layers = []\n",
    "        f = open(file_path, \"r\")       \n",
    "        name = f.readline().split()[0]\n",
    "        total_n_of_layers = int(f.readline().split()[0])\n",
    "        input_layer_size = int(f.readline().split()[0])\n",
    "\n",
    "        in_layer = input_layer(input_layer_size)\n",
    "        layers.append(in_layer)\n",
    "        \n",
    "        s.give_name(name)\n",
    "\n",
    "        for i in range(total_n_of_layers-1):\n",
    "            prev_size = int(f.readline().split()[0])\n",
    "            size = int(f.readline().split()[0])\n",
    "            weights = np.loadtxt(f, max_rows = size)\n",
    "            bias = np.loadtxt(f, max_rows = size)\n",
    "            activation_type = f.readline().split()[0]\n",
    "            lr = float(f.readline().split()[0])\n",
    "\n",
    "            layers.append(layer(lr = lr, prev_size = prev_size, my_size = size, \n",
    "                                activation_type = activation_type, weights = weights, bias = bias))\n",
    "            \n",
    "        s.append_layers(layers)             \n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d1c3a-2d27-43f4-a307-5f7bf7585e54",
   "metadata": {},
   "source": [
    "## TESt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b39afe9-558e-415d-9b4a-c4ac322dfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60dcf020-3912-4dc3-9ac4-3fd53d20c072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learning_rate_a = 0.0001\n",
    "# in_states = 3\n",
    "# h1_nodes = 7\n",
    "# h2_nodes = 7\n",
    "# out_actions = 5\n",
    "\n",
    "# a_type1 = 'ReLU'\n",
    "# a_type2 = 'Linear'\n",
    "\n",
    "# layers_net = [input_layer(in_states), \n",
    "# layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type=a_type1), \n",
    "# #layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type=a_type1), \n",
    "# layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type=a_type2)]\n",
    "\n",
    "# target_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "# target_dqn.append_layers(layers_net)\n",
    "\n",
    "# target_dqn.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e8e0b2-f620-41e8-b5c9-b9ba05656e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add_layer = layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type=a_type2)\n",
    "# target_dqn.delete_layer_by_pos(1)\n",
    "# target_dqn.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef9bf3b7-a3f2-4efe-943e-b91cd6ceeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN():\n",
    "    # Hyperparameters (adjustable)\n",
    "    def __init__(s, ct = 0, tag='tag', path = r'test', \n",
    "                 game_name = 'MountainCar-v0', \n",
    "                  \n",
    "                 mini_batch_size = 32,  num_divisions = 1, \n",
    "                 replay_memory_size = 100000, \n",
    "                 network_sync_rate = 50000, discount_factor_g = 0.9):\n",
    "        \n",
    "        if (ct == 0):\n",
    "            ct = datetime.datetime.now()\n",
    "            ct = str(ct)\n",
    "            ct = ct.replace(\":\", \"-\")\n",
    "            ct = ct.replace(\" \", \"_\")\n",
    "            ct = ct[:-7]\n",
    "\n",
    "        s.ct = ct\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory_pics = os.path.join(current_directory, s.ct)\n",
    "        \n",
    "        final_directory_pics = os.path.join(final_directory_pics, 'pics')\n",
    "        if not os.path.exists(final_directory_pics):\n",
    "            os.makedirs(final_directory_pics)\n",
    "            \n",
    "        s.set_tag(tag)\n",
    "\n",
    "        s.game_name = game_name\n",
    "        s.discount_factor_g = discount_factor_g\n",
    "         \n",
    "        s.mini_batch_size = mini_batch_size \n",
    "        s.num_divisions = num_divisions\n",
    "\n",
    "        s.replay_memory_size =  replay_memory_size \n",
    "        s.network_sync_rate = network_sync_rate\n",
    "        \n",
    "    def set_tag(s, tag):\n",
    "        s.tag = tag\n",
    "        s.path = s.ct+'/'+s.tag\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory = os.path.join(current_directory, s.path)\n",
    "        if not os.path.exists(final_directory):\n",
    "            os.makedirs(final_directory)\n",
    "        \n",
    "    \n",
    "    def plot_progress(self, rewards_per_episode_, epsilon_history_):\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('reward')\n",
    "        plt.plot(rewards_per_episode_)\n",
    "\n",
    "        plt.savefig(f'{self.path}/info_rew_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('epsilon')\n",
    "        plt.plot(epsilon_history_)\n",
    "        plt.savefig(f'{self.path}/info_eps_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, policy_dqn, target_dqn, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "\n",
    "        #env.action_space.seed(42)\n",
    "        \n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        rewards_per_episode = []\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count = 0\n",
    "        goal_reached = False\n",
    "        best_rewards = -200\n",
    "\n",
    "        for i in range(episodes+1):\n",
    "            state = env.reset()[0]  # Initialize to state 0 seed=int(i+10)\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False\n",
    "            rewards = 0\n",
    "\n",
    "            while(not terminated and rewards > -1000):\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    res = policy_dqn.forward(self.state_to_dqn_input(state))\n",
    "                    action = res.argmax().item()\n",
    "\n",
    "# Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "\n",
    "#Truncation: The length of the episode is 200.\n",
    "\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "                rewards += reward\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "                state = new_state\n",
    "                \n",
    "                step_count+=1\n",
    "\n",
    "\n",
    "            rewards_per_episode.append(rewards)\n",
    "            \n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                self.add_log_data(f'Episode {i} Epsilon {epsilon}')\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                self.add_log_data(f'Best rewards so far: {best_rewards}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_best'.format(self.path))\n",
    "                \n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size and goal_reached:\n",
    "                \n",
    "                #print(f'OPTIMIZE Episode {i} Epsilon {epsilon} rewards {rewards}') # print(rewards)\n",
    "\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = epsilon = max(epsilon - 1/episodes, 0.01) # max(epsilon*0.99996, 0.05)#\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn = policy_dqn.deep_copy()\n",
    "                    step_count = 0\n",
    "                   \n",
    "                \n",
    "                \n",
    "        env.close()\n",
    "        policy_dqn.save(f'{self.path}/mc_policy_last_{self.tag}'.format(self.path, self.tag))\n",
    "        if (best_rewards == -200):\n",
    "            policy_dqn.save(f'{self.path}/mc_policy_best'.format(self.path))\n",
    "        self.save_reward_data(rewards_per_episode)\n",
    "        #print(rewards_per_episode)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        target_q_list = []\n",
    "        input_list = []\n",
    "        \n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor_g * target_dqn.forward(self.state_to_dqn_input(new_state)).max()\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            state_dsc = np.asarray(self.state_to_dqn_input(state))\n",
    "            input_list.append(state_dsc)\n",
    "            \n",
    "            target_q = target_dqn.forward(state_dsc)\n",
    "            \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target            \n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        #BACKPOP AND UPDATE on minibatch\n",
    "        x = np.asarray(input_list)\n",
    "        x = x[:, :, 0]\n",
    "        x = x.T\n",
    "\n",
    "        y = np.asarray(target_q_list)\n",
    "        y = y[:, :, 0]\n",
    "        y = y.T\n",
    "\n",
    "        policy_dqn.learn_one(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_dqn_input(self, state):\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        \n",
    "        # state_p = state[0]\n",
    "        # state_v = state[1]\n",
    "\n",
    "        return np.asarray([[state_p], [state_v]])\n",
    "\n",
    "        \n",
    "\n",
    "    def test(self, policy_dqn, episodes, render = False):\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "        \n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        done_count = 0\n",
    "        medium_reward = 0\n",
    "        reward_list = []\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            state, info = env.reset()  # Initialize to state 0\n",
    "            done = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "            rewards = 0\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            \n",
    "            while(not done and rewards > -500):\n",
    "            #while(not done and not truncated):\n",
    "                state = self.state_to_dqn_input(state)\n",
    "                #print('state shape', state.shape)\n",
    "               \n",
    "                res = policy_dqn.forward(state)\n",
    "\n",
    "                action = res.argmax().item()\n",
    "                \n",
    "# Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "\n",
    "#Truncation: The length of the episode is 200.\n",
    "\n",
    "                #new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "                \n",
    "                state, reward, done, truncated, _ = env.step(action)\n",
    "                rewards+=reward\n",
    "                if (done):\n",
    "                    done_count += 1\n",
    "                    break\n",
    "\n",
    "            medium_reward += rewards\n",
    "            reward_list.append(rewards)\n",
    "                \n",
    "\n",
    "        \n",
    "        env.close()\n",
    "        medium_reward = medium_reward / episodes\n",
    "        return done_count*100.0/episodes, medium_reward, reward_list\n",
    "        \n",
    "    def save_info(s, info):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\")   \n",
    "       \n",
    "\n",
    "\n",
    "        f.write(\"data {}\\n\".format(s.ct))\n",
    "        f.write(\"tag {}\\n\".format(s.tag))\n",
    "        f.write(\"game_name {}\\n\".format(s.game_name))\n",
    "        f.write(\"reward discount factor {}\\n\".format(s.discount_factor_g))\n",
    "        f.write(\"minibatch size {}\\n\".format(s.mini_batch_size))\n",
    "        f.write(\"num divisions{}\\n\".format(s.num_divisions))\n",
    "        f.write(\"replay memory size {}\\n\".format(s.replay_memory_size))\n",
    "        f.write(\"network sync rate {}\\n\".format(s.network_sync_rate))\n",
    "        f.write(\"info {}\\n\".format(info))\n",
    "                         \n",
    "        f.close()\n",
    "    \n",
    "    def add_log_data(s, data):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n",
    "        \n",
    "    def save_reward_data(s, data):\n",
    "        file_path = f'{s.path}/rewards_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adee0135-c018-49e9-989d-f3c25a773532",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_a = 0.001\n",
    "in_states = 2\n",
    "h1_nodes = 3\n",
    "h2_nodes = 3\n",
    "out_actions = 3\n",
    "\n",
    "a_type1 = 'ReLU'\n",
    "a_type2 = 'Linear'\n",
    "\n",
    "layers_net = [input_layer(in_states), \n",
    "layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type=a_type1), \n",
    "layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type=a_type1), \n",
    "layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type=a_type2)]\n",
    "\n",
    "policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "policy_dqn.append_layers(layers_net)\n",
    "\n",
    "target_dqn = plastic_nn()\n",
    "target_dqn = policy_dqn.deep_copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29aae411-5eea-4be8-9684-2c0ac9402a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mountaincar = DQN(game_name = 'MountainCar-v0', discount_factor_g = 0.9, mini_batch_size = 64, \n",
    "                  num_divisions = 50, replay_memory_size = 100000, network_sync_rate = 50000)\n",
    "mountaincar.set_tag('00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1b2dc3d-b43a-4985-a043-de1064654d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 Epsilon 1\n",
      "Episode 2000 Epsilon 1\n",
      "Episode 3000 Epsilon 1\n",
      "Episode 4000 Epsilon 0.8942000000000117\n",
      "Episode 5000 Epsilon 0.6942000000000337\n"
     ]
    }
   ],
   "source": [
    "mountaincar.save_info(f'lr: {learning_rate_a} \\nin:{in_states} \\nh:{h1_nodes}x2 (--) \\nout:{out_actions} \\na1:{a_type1} \\na2:{a_type2} \\n'.format(\n",
    "    learning_rate_a, in_states, h1_nodes, out_actions, a_type1, a_type2))\n",
    "mountaincar.train(policy_dqn, target_dqn, 5000, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "178ef56e-4a42-49d2-93fd-4988b1fdd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountaincar.set_tag('01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62ffbe46-690e-4ae0-b926-57573fb2f39a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:  noname  ( 4 )\n",
      "# 0\n",
      "IN LAYER\n",
      "size:  2\n",
      "\n",
      "# 1\n",
      "my size:  3\n",
      "prev size:  2\n",
      "w:  (3, 2) [[ 0.64758615  0.03189151]\n",
      " [ 0.35066547  0.64768358]\n",
      " [-0.10836567  0.12662346]] \n",
      "\n",
      "b:  (3, 1) [[-0.07047263]\n",
      " [ 0.51074756]\n",
      " [ 0.66746286]] \n",
      "\n",
      "\n",
      "# 2\n",
      "my size:  3\n",
      "prev size:  3\n",
      "w:  (3, 3) [[-0.07386445  0.78649327  0.16749688]\n",
      " [ 0.03803286  0.38445125 -0.02373191]\n",
      " [ 0.63849218  0.0528816   0.69475779]] \n",
      "\n",
      "b:  (3, 1) [[ 0.50139307]\n",
      " [-0.2119452 ]\n",
      " [-0.04805645]] \n",
      "\n",
      "\n",
      "# 3\n",
      "my size:  3\n",
      "prev size:  3\n",
      "w:  (3, 3) [[ 0.0185373   0.64281852  0.27252808]\n",
      " [ 0.19867912 -0.01490822  0.52957165]\n",
      " [ 0.05522255  0.58953189  0.25340823]] \n",
      "\n",
      "b:  (3, 1) [[0.70239928]\n",
      " [0.41880444]\n",
      " [0.47917633]] \n",
      "\n",
      "\n",
      "NAME:  noname  ( 4 )\n",
      "# 0\n",
      "IN LAYER\n",
      "size:  2\n",
      "\n",
      "# 1\n",
      "my size:  4\n",
      "prev size:  2\n",
      "w:  (4, 2) [[ 0.64758615  0.03189151]\n",
      " [ 0.35066547  0.64768358]\n",
      " [-0.10836567  0.12662346]\n",
      " [ 0.1         0.1       ]] \n",
      "\n",
      "b:  (4, 1) [[-0.07047263]\n",
      " [ 0.51074756]\n",
      " [ 0.66746286]\n",
      " [ 0.1       ]] \n",
      "\n",
      "\n",
      "# 2\n",
      "my size:  4\n",
      "prev size:  4\n",
      "w:  (4, 4) [[-0.07386445  0.78649327  0.16749688  0.1       ]\n",
      " [ 0.03803286  0.38445125 -0.02373191  0.1       ]\n",
      " [ 0.63849218  0.0528816   0.69475779  0.1       ]\n",
      " [ 0.1         0.1         0.1         0.1       ]] \n",
      "\n",
      "b:  (4, 1) [[ 0.50139307]\n",
      " [-0.2119452 ]\n",
      " [-0.04805645]\n",
      " [ 0.1       ]] \n",
      "\n",
      "\n",
      "# 3\n",
      "my size:  3\n",
      "prev size:  4\n",
      "w:  (3, 4) [[ 0.0185373   0.64281852  0.27252808  0.1       ]\n",
      " [ 0.19867912 -0.01490822  0.52957165  0.1       ]\n",
      " [ 0.05522255  0.58953189  0.25340823  0.1       ]] \n",
      "\n",
      "b:  (3, 1) [[0.70239928]\n",
      " [0.41880444]\n",
      " [0.47917633]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_dqn.print_info()\n",
    "\n",
    "policy_dqn.add_neuron(1)\n",
    "policy_dqn.add_neuron(2)\n",
    "policy_dqn.print_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a629039d-7cbf-4689-8337-44c27dc2a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dqn = policy_dqn.deep_copy()\n",
    "policy_dqn.optimizer_reset()\n",
    "target_dqn.optimizer_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e598c74-966e-426d-a530-94ad0bbd6a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 Epsilon 0.8154000000000203\n",
      "Episode 2000 Epsilon 0.6154000000000424\n",
      "Episode 3000 Epsilon 0.4154000000000644\n",
      "Episode 4000 Epsilon 0.2154000000000816\n"
     ]
    }
   ],
   "source": [
    "mountaincar.save_info(f'lr: {learning_rate_a} \\nin:{in_states} \\nh:{h1_nodes}x2 (--) \\nout:{out_actions} \\na1:{a_type1} \\na2:{a_type2} \\n'.format(\n",
    "    learning_rate_a, in_states, 4, out_actions, a_type1, a_type2))\n",
    "mountaincar.train(policy_dqn, target_dqn, 5000, False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
