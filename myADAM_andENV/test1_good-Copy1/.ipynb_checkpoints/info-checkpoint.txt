Episode 1000 Epsilon 0.9660500000000037
Episode 2000 Epsilon 0.9160500000000092
Episode 3000 Epsilon 0.8660500000000148
Episode 4000 Epsilon 0.8160500000000203
Episode 5000 Epsilon 0.7660500000000258
Episode 6000 Epsilon 0.7160500000000313
Episode 7000 Epsilon 0.6660500000000368
Episode 8000 Epsilon 0.6160500000000423
Episode 9000 Epsilon 0.5660500000000478
Episode 10000 Epsilon 0.5160500000000533
Episode 11000 Epsilon 0.4660500000000588
Episode 12000 Epsilon 0.4160500000000643
Episode 13000 Epsilon 0.3660500000000698
Episode 14000 Epsilon 0.3160500000000753
Best rewards so far: -196.0
Episode 15000 Epsilon 0.26605000000008083
Episode 16000 Epsilon 0.21605000000008634
Episode 17000 Epsilon 0.16605000000009185
Episode 18000 Epsilon 0.11605000000009735
Episode 19000 Epsilon 0.06605000000010286


learning_rate_a = 0.01
in_states = 2
h1_nodes = 20
h2_nodes = 20
out_actions = 3

layers_net = [input_layer(in_states), 
layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type="ReLU"), 
#layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type="ReLU"), 
layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type='Linear')]

policy_dqn = plastic_nn(optimizer="Adam")
policy_dqn.append_layers(layers_net)

target_dqn = plastic_nn()
target_dqn = policy_dqn.deep_copy()

discount_factor_g = 0.9         # discount rate (gamma)
    network_sync_rate = 50000          # number of steps the agent takes before syncing the policy and target network
    replay_memory_size = 100000       # size of replay memory
    mini_batch_size = 64            # size of the training data set sampled from the replay memory

    num_divisions = 50