{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K5feZBXDqSyO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5feZBXDqSyO",
    "outputId": "f8fa2bd4-7df7-40c6-97a6-3a71739035a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e9d45-e28b-4603-9058-3260849a1668",
   "metadata": {
    "id": "290e9d45-e28b-4603-9058-3260849a1668"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "from TrulyPlastic_allOpt import plastic_nn\n",
    "from TrulyPlastic_allOpt import input_layer\n",
    "from TrulyPlastic_allOpt import layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea29ee5-b349-4619-8137-3d460e672f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea6a06-3557-4e61-a769-420d9de43ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN():\n",
    "    # Hyperparameters (adjustable)\n",
    "    def __init__(s, ct, path = r'test', game_name = 'MountainCar-v0', discount_factor_g = 0.9, mini_batch_size = 64, \n",
    "                  num_divisions = 50, replay_memory_size = 100000, network_sync_rate = 50000):\n",
    "        s.ct = ct\n",
    "        \n",
    "        s.path = path\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory = os.path.join(current_directory, s.path)\n",
    "        if not os.path.exists(final_directory):\n",
    "            os.makedirs(final_directory)\n",
    "    \n",
    "        s.game_name = game_name\n",
    "        s.discount_factor_g = discount_factor_g\n",
    "         \n",
    "        s.mini_batch_size = mini_batch_size \n",
    "        s.num_divisions = num_divisions\n",
    "\n",
    "        s.replay_memory_size =  replay_memory_size \n",
    "        s.network_sync_rate = network_sync_rate\n",
    "        \n",
    "    \n",
    "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.subplot(121) \n",
    "        plt.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('reward')\n",
    "        plt.plot(rewards_per_episode)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('epsilon')\n",
    "        plt.plot(epsilon_history)\n",
    "        plt.savefig(f'{self.path}/info_{self.ct}.png'.format(self.path, self.ct))\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, policy_dqn, target_dqn, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        rewards_per_episode = []\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count = 0\n",
    "        goal_reached = False\n",
    "        best_rewards = -200\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False\n",
    "            rewards = 0\n",
    "\n",
    "            while(not terminated and rewards > -1000):\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    res = policy_dqn.forward(self.state_to_dqn_input(state))\n",
    "                    action = res.argmax().item()\n",
    "\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "                rewards += reward\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "                state = new_state\n",
    "                step_count+=1\n",
    "\n",
    "\n",
    "            rewards_per_episode.append(rewards)\n",
    "            \n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                \n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size and goal_reached:\n",
    "                \n",
    "                #print(f'OPTIMIZE Episode {i} Epsilon {epsilon} rewards {rewards}') # print(rewards)\n",
    "                \n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = epsilon = max(epsilon - 1/episodes, 0.01) # max(epsilon*0.99996, 0.05)#\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn = policy_dqn.deep_copy()\n",
    "                    step_count = 0\n",
    "                   \n",
    "                \n",
    "                \n",
    "        env.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        target_q_list = []\n",
    "        input_list = []\n",
    "        \n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor_g * target_dqn.forward(self.state_to_dqn_input(new_state)).max()\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            state_dsc = np.asarray(self.state_to_dqn_input(state))\n",
    "            input_list.append(state_dsc)\n",
    "            \n",
    "            target_q = target_dqn.forward(state_dsc)\n",
    "            \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target            \n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        #BACKPOP AND UPDATE on minibatch\n",
    "        x = np.asarray(input_list)\n",
    "        x = x[:, :, 0]\n",
    "        x = x.T\n",
    "\n",
    "        y = np.asarray(target_q_list)\n",
    "        y = y[:, :, 0]\n",
    "        y = y.T\n",
    "\n",
    "        policy_dqn.learn_one(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_dqn_input(self, state):\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "\n",
    "        return np.asarray([[state_p], [state_v]])\n",
    "\n",
    "        \n",
    "\n",
    "    def test(self, policy_dqn, episodes, render = False):\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "        \n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        done_count = 0\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            state, info = env.reset()  # Initialize to state 0\n",
    "            done = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not done and not truncated):\n",
    "                state = self.state_to_dqn_input(state)\n",
    "                #print('state shape', state.shape)\n",
    "               \n",
    "                res = policy_dqn.forward(state)\n",
    "\n",
    "                action = res.argmax().item()\n",
    "                #print(res)\n",
    "                state, reward, done, truncated, _ = env.step(action)\n",
    "                if (done):\n",
    "                    done_count += 1\n",
    "\n",
    "        print(done_count/episodes)\n",
    "        env.close()\n",
    "\n",
    "    def save_info(s, ct, topology):\n",
    "        file_path = f'{s.path}/info.txt'.format(s.path)\n",
    "        f = open(file_path, \"a\")   \n",
    "       \n",
    "\n",
    "\n",
    "        f.write(\"{}\\n\".format(s.ct))\n",
    "        \n",
    "        f.write(\"{}\\n\".format(s.game_name))\n",
    "        f.write(\"{}\\n\".format(s.discount_factor_g))\n",
    "        f.write(\"{}\\n\".format(s.mini_batch_size))\n",
    "        f.write(\"{}\\n\".format(s.num_divisions))\n",
    "        f.write(\"{}\\n\".format(s.replay_memory_size))\n",
    "        f.write(\"{}\\n\".format(s.network_sync_rate))\n",
    "        f.write(\"{}\\n\".format(topology))\n",
    "                         \n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045c962-6024-43b8-8527-1376ce918872",
   "metadata": {
    "id": "f045c962-6024-43b8-8527-1376ce918872",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate_a = 0.0001\n",
    "in_states = 2\n",
    "h1_nodes = 10\n",
    "h2_nodes = 10\n",
    "out_actions = 3\n",
    "\n",
    "a_type1 = 'ReLU'\n",
    "a_type2 = 'Linear'\n",
    "\n",
    "layers_net = [input_layer(in_states), \n",
    "layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type=a_type1), \n",
    "#layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type=\"ReLU\"), \n",
    "layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type=a_type2)]\n",
    "\n",
    "policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "policy_dqn.append_layers(layers_net)\n",
    "\n",
    "target_dqn = plastic_nn()\n",
    "target_dqn = policy_dqn.deep_copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8eaab-4e8c-497b-9cfc-0dd2cf1f9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now()\n",
    "ct = str(ct)\n",
    "ct = ct.replace(\":\", \"-\")\n",
    "ct = ct.replace(\" \", \"_\")\n",
    "ct = ct[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219350b2-41b7-44de-b105-8ab7aefd85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mountaincar = DQN(ct, path = ct, game_name = 'MountainCar-v0', discount_factor_g = 0.9, mini_batch_size = 32, \n",
    "                  num_divisions = 20, replay_memory_size = 100000, network_sync_rate = 50000)\n",
    "mountaincar.save_info(ct, f'lr: {learning_rate_a} \\nin:{in_states} \\nh:{h1_nodes} \\nout:{out_actions} \\na1:{a_type1} \\na2:{a_type2} \\n'.format(\n",
    "    learning_rate_a, in_states, h1_nodes, out_actions, a_type1, a_type2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
    "outputId": "5d51151e-415f-4612-c064-a8a6a337d5d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mountaincar.train(policy_dqn, target_dqn, 20000, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c628ab-e67d-4826-b24c-d7fbd73f8366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a07137-0867-4bfd-a11e-bc208a3cee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_dqn.load('test2/mc_policy_9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
   "metadata": {
    "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mountaincar.test(policy_dqn, 4, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf01c5-f2fc-48c3-aee3-27e1ff536917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_dqn.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea8963d-2e57-46f6-b2ba-35c481ea6423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_dqn.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f4d8e-fac6-4220-9687-ded89638b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[10, 11], [6, 6], [9, 14], [7, 10], [8, 12], [7, 10], [7, 9], [9, 10], [8, 9], [8, 9], [9, 12], [7, 8], [5, 10], [7, 10], [10, 12], [9, 12], [7, 9], [8, 11], [7, 9], [11, 9], [7, 7], [10, 9], [8, 11], [10, 11], [7, 13], [9, 10], [8, 9], [8, 9], [8, 10], [8, 10], [9, 11], [6, 16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9495f-2c6b-4452-96a5-f551096145ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b85271-fa23-4b2b-8c9a-a577ce016c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c78fedb0-800f-495b-b18b-2e3fa0acc238",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
