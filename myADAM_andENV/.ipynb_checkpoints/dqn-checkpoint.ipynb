{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "K5feZBXDqSyO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5feZBXDqSyO",
    "outputId": "f8fa2bd4-7df7-40c6-97a6-3a71739035a2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (7.1.0)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (2.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290e9d45-e28b-4603-9058-3260849a1668",
   "metadata": {
    "id": "290e9d45-e28b-4603-9058-3260849a1668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from TrulyPlastic_allOpt.ipynb\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import import_ipynb\n",
    "from TrulyPlastic_allOpt import plastic_nn\n",
    "from TrulyPlastic_allOpt import input_layer\n",
    "from TrulyPlastic_allOpt import layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ea29ee5-b349-4619-8137-3d460e672f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cea6a06-3557-4e61-a769-420d9de43ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MountainCarDQL():\n",
    "    # Hyperparameters (adjustable)\n",
    "    \n",
    "    discount_factor_g = 0.9         # discount rate (gamma)\n",
    "    network_sync_rate = 50000          # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 100000       # size of replay memory\n",
    "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "    num_divisions = 20\n",
    "\n",
    "    \n",
    "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
    "        plt.figure(1)\n",
    "        plt.subplot(121) \n",
    "        plt.plot(rewards_per_episode)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "        plt.savefig('mountaincar_dql.png')\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, policy_dqn, target_dqn, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        rewards_per_episode = []\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count = 0\n",
    "        goal_reached = False\n",
    "        best_rewards = -200\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False\n",
    "            rewards = 0\n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and rewards > -1000):\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    res = policy_dqn.forward(self.state_to_dqn_input(state))\n",
    "                    #print(res)\n",
    "                    action = res.argmax().item()\n",
    "\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "                rewards += reward\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "                state = new_state\n",
    "                step_count+=1\n",
    "\n",
    "\n",
    "            rewards_per_episode.append(rewards)\n",
    "            #epsilon_history.append(epsilon)\n",
    "            \n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "                policy_dqn.save(f'mc_policy_{i}'.format(i))\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                policy_dqn.save(f'mc_policy_{i}'.format(i))\n",
    "                \n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size and goal_reached:\n",
    "                \n",
    "                #print(f'OPTIMIZE Episode {i} Epsilon {epsilon} rewards {rewards}') # print(rewards)\n",
    "                \n",
    "                mini_batch = memory.sample(self.mini_batch_size) #len(memory))#\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = epsilon = max(epsilon - 1/episodes, 0.01) # max(epsilon*0.99996, 0.05)#\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn = policy_dqn.deep_copy()\n",
    "                    step_count = 0\n",
    "                    #print('UPDATE')\n",
    "                   \n",
    "                \n",
    "                \n",
    "        env.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "        input_list = []\n",
    "\n",
    "        \n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor_g * target_dqn.forward(self.state_to_dqn_input(new_state)).max()\n",
    "          \n",
    "            # Get the current set of Q values\n",
    "            # current_q = policy_dqn.forward(self.state_to_dqn_input(state))\n",
    "            # current_q_list.append(current_q)\n",
    "            in_state_dsc = np.asarray(self.state_to_dqn_input(state))\n",
    "            \n",
    "            input_list.append(in_state_dsc)\n",
    "\n",
    "            \n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn.forward(in_state_dsc)\n",
    "            \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            #print(in_state_dsc.shape)\n",
    "            \n",
    "            target_q_list.append(target_q)\n",
    "            #target_q_list = np.asarray(target_q_list)\n",
    "            #print(target_q_list.shape)\n",
    "            \n",
    "\n",
    "    \n",
    "        #BACKPOP AND UPDATE on minibatch\n",
    "        # print(input_list)\n",
    "        # print(target_q)\n",
    "        x = np.asarray(input_list)\n",
    "        x = x[:, :, 0]\n",
    "        x = x.T\n",
    "        print(x.shape)\n",
    "        y = np.asarray(target_q_list)\n",
    "        y = y[:, :, 0]\n",
    "        y = y.T\n",
    "        #print(x.shape)\n",
    "        #print(target_q)\n",
    "        #print('target_q_list ',target_q_list.shape)\n",
    "        policy_dqn.learn_one(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_dqn_input(self, state):\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "\n",
    "        return np.asarray([[state_p], [state_v]])\n",
    "\n",
    "    # Run the environment with the learned policy\n",
    "    def test(self, policy_dqn, episodes):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state, info = env.reset()  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "                state = self.state_to_dqn_input(state)\n",
    "                print(state)\n",
    "                res = policy_dqn.forward(state)\n",
    "\n",
    "                action = res.argmax().item()\n",
    "\n",
    "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "                # dscr = np.asarray(self.state_to_dqn_input(state))\n",
    "                # print(dscr.shape)\n",
    "                # #dscr = dscr.reshape(dscr[0], 1)\n",
    "                # res = policy_dqn.forward(dscr)\n",
    "                # #print(res)\n",
    "                # action = res.argmax().item()\n",
    "\n",
    "                # # Execute action\n",
    "                # state,reward,terminated,truncated,_ = env.step(action)\n",
    "                #terminated = True\n",
    "\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f045c962-6024-43b8-8527-1376ce918872",
   "metadata": {
    "id": "f045c962-6024-43b8-8527-1376ce918872",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added LAYERS succesfully\n"
     ]
    }
   ],
   "source": [
    "learning_rate_a = 0.0001\n",
    "in_states = 2\n",
    "h1_nodes = 10\n",
    "h2_nodes = 10\n",
    "out_actions = 3\n",
    "\n",
    "layers_net = [input_layer(in_states), \n",
    "layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type=\"ReLU\"), \n",
    "#layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type=\"ReLU\"), \n",
    "layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type='Linear')]\n",
    "\n",
    "policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "policy_dqn.append_layers(layers_net)\n",
    "\n",
    "target_dqn = plastic_nn()\n",
    "target_dqn = policy_dqn.deep_copy()\n",
    "#a = np.asarray([[11], [12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "219350b2-41b7-44de-b105-8ab7aefd85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountaincar = MountainCarDQL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
    "outputId": "5d51151e-415f-4612-c064-a8a6a337d5d1",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmountaincar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 77\u001b[0m, in \u001b[0;36mMountainCarDQL.train\u001b[1;34m(self, policy_dqn, target_dqn, episodes, render)\u001b[0m\n\u001b[0;32m     75\u001b[0m new_state,reward,terminated,truncated,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     76\u001b[0m rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 77\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m((state, action, new_state, reward, terminated))\n\u001b[0;32m     78\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m     79\u001b[0m step_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mountaincar.train(policy_dqn, target_dqn, 20000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a07137-0867-4bfd-a11e-bc208a3cee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added LAYERS succesfully\n"
     ]
    }
   ],
   "source": [
    "policy_dqn.load('mc_policy_12000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
   "metadata": {
    "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7]\n",
      " [10]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,10) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmountaincar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_dqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 200\u001b[0m, in \u001b[0;36mMountainCarDQL.test\u001b[1;34m(self, policy_dqn, episodes)\u001b[0m\n\u001b[0;32m    198\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_to_dqn_input(state)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m--> 200\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_dqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m action \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    204\u001b[0m new_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m<string>:34\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(s, x, to_print)\u001b[0m\n",
      "File \u001b[1;32m<string>:40\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(s, x, to_print)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,10) (3,) "
     ]
    }
   ],
   "source": [
    "mountaincar.test(policy_dqn, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ea8963d-2e57-46f6-b2ba-35c481ea6423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:  noname  ( 3 )\n",
      "# 0\n",
      "IN LAYER\n",
      "size:  2\n",
      "\n",
      "# 1\n",
      "my size:  10\n",
      "prev size:  2\n",
      "w:  [[ 0.48723485 -0.00786437]\n",
      " [ 0.52588495  0.17287111]\n",
      " [ 0.29225033 -0.20633994]\n",
      " [ 1.41306725  0.61503352]\n",
      " [-0.12788561  0.06082077]\n",
      " [ 0.47800021 -0.07948325]\n",
      " [ 0.37128183  0.40662291]\n",
      " [-0.25416424  0.09465061]\n",
      " [-0.28624286 -0.01088488]\n",
      " [-0.39746061  0.4429162 ]] \n",
      "\n",
      "b:  [[ 0.48723485 -0.00786437]\n",
      " [ 0.52588495  0.17287111]\n",
      " [ 0.29225033 -0.20633994]\n",
      " [ 1.41306725  0.61503352]\n",
      " [-0.12788561  0.06082077]\n",
      " [ 0.47800021 -0.07948325]\n",
      " [ 0.37128183  0.40662291]\n",
      " [-0.25416424  0.09465061]\n",
      " [-0.28624286 -0.01088488]\n",
      " [-0.39746061  0.4429162 ]] \n",
      "\n",
      "\n",
      "# 2\n",
      "my size:  3\n",
      "prev size:  10\n",
      "w:  [[ 0.07880055  0.42727197  0.21839219 -0.55088046 -0.1794802   0.19604556\n",
      "  -0.01120691  0.28302058  0.1799719   0.0956271 ]\n",
      " [-0.05370799  0.11437935  0.46358822 -0.44989954  0.02800195  0.14080314\n",
      "   0.14421809 -0.10143946  0.37320969  0.02981011]\n",
      " [ 0.42097798 -0.15798436  0.35403567 -0.47707248  0.18335917 -0.20734177\n",
      "   0.37235015  0.11777445  0.64734753 -0.10299763]] \n",
      "\n",
      "b:  [[ 0.07880055  0.42727197  0.21839219 -0.55088046 -0.1794802   0.19604556\n",
      "  -0.01120691  0.28302058  0.1799719   0.0956271 ]\n",
      " [-0.05370799  0.11437935  0.46358822 -0.44989954  0.02800195  0.14080314\n",
      "   0.14421809 -0.10143946  0.37320969  0.02981011]\n",
      " [ 0.42097798 -0.15798436  0.35403567 -0.47707248  0.18335917 -0.20734177\n",
      "   0.37235015  0.11777445  0.64734753 -0.10299763]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_dqn.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961f4d8e-fac6-4220-9687-ded89638b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[10, 11], [6, 6], [9, 14], [7, 10], [8, 12], [7, 10], [7, 9], [9, 10], [8, 9], [8, 9], [9, 12], [7, 8], [5, 10], [7, 10], [10, 12], [9, 12], [7, 9], [8, 11], [7, 9], [11, 9], [7, 7], [10, 9], [8, 11], [10, 11], [7, 13], [9, 10], [8, 9], [8, 9], [8, 10], [8, 10], [9, 11], [6, 16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9495f-2c6b-4452-96a5-f551096145ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b85271-fa23-4b2b-8c9a-a577ce016c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c78fedb0-800f-495b-b18b-2e3fa0acc238",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
