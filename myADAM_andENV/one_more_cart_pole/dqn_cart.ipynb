{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "K5feZBXDqSyO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5feZBXDqSyO",
    "outputId": "f8fa2bd4-7df7-40c6-97a6-3a71739035a2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (7.1.0)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (2.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "290e9d45-e28b-4603-9058-3260849a1668",
   "metadata": {
    "id": "290e9d45-e28b-4603-9058-3260849a1668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from TrulyPlastic_allOpt_cart.ipynb\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "from TrulyPlastic_allOpt_cart import plastic_nn\n",
    "from TrulyPlastic_allOpt_cart import input_layer\n",
    "from TrulyPlastic_allOpt_cart import layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cea6a06-3557-4e61-a769-420d9de43ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN():\n",
    "    # Hyperparameters (adjustable)\n",
    "    def __init__(s, ct, tag='tag',path = r'test', game_name = 'MountainCar-v0', discount_factor_g = 0.9, \n",
    "                 mini_batch_size = 32, \n",
    "                  num_divisions = 20, replay_memory_size = 100000, network_sync_rate = 50000):\n",
    "        s.ct = ct\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory_pics = os.path.join(current_directory, s.ct)\n",
    "        \n",
    "        final_directory_pics = os.path.join(final_directory_pics, 'pics')\n",
    "        if not os.path.exists(final_directory_pics):\n",
    "            os.makedirs(final_directory_pics)\n",
    "            \n",
    "        \n",
    "        s.tag = tag\n",
    "        s.path = s.ct+'/'+s.tag\n",
    "        current_directory = os.getcwd()\n",
    "        final_directory = os.path.join(current_directory, s.path)\n",
    "        if not os.path.exists(final_directory):\n",
    "            os.makedirs(final_directory)\n",
    "            \n",
    "\n",
    "    \n",
    "        s.game_name = game_name\n",
    "        s.discount_factor_g = discount_factor_g\n",
    "         \n",
    "        s.mini_batch_size = mini_batch_size \n",
    "        s.num_divisions = num_divisions\n",
    "\n",
    "        s.replay_memory_size =  replay_memory_size \n",
    "        s.network_sync_rate = network_sync_rate\n",
    "        \n",
    "    \n",
    "    def plot_progress(self, rewards_per_episode_, epsilon_history_):\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('reward')\n",
    "        plt.plot(rewards_per_episode_)\n",
    "\n",
    "        plt.savefig(f'{self.path}/info_rew_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.savefig(f'{self.ct}/pics/info_rew_{self.tag}.png')\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('epsilon')\n",
    "        plt.plot(epsilon_history_)\n",
    "        plt.savefig(f'{self.path}/info_eps_{self.tag}.png'.format(self.path, self.tag))\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, policy_dqn, target_dqn, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "\n",
    "        #env.action_space.seed(42)\n",
    "        \n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        #self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        #self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        rewards_per_episode = []\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count = 0\n",
    "        goal_reached = False\n",
    "        best_rewards = -200\n",
    "\n",
    "        for i in range(episodes+1):\n",
    "            state = env.reset()[0]  # Initialize to state 0 seed=int(i+10)\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False\n",
    "            rewards = 0\n",
    "\n",
    "            while (not terminated and rewards < 300):\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    res = policy_dqn.forward(self.state_to_dqn_input(state))\n",
    "                    action = res.argmax().item()\n",
    "\n",
    "#Termination: Pole Angle is greater than ±12°\n",
    "#Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "#Truncation: Episode length is greater than 500 (200 for v0)\n",
    " \n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "                rewards += reward\n",
    "                # if (terminated):\n",
    "                #     reward = -1\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "                state = new_state\n",
    "                \n",
    "                step_count+=1\n",
    "\n",
    "\n",
    "            rewards_per_episode.append(rewards)\n",
    "            \n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                self.add_log_data(f'Episode {i} Epsilon {epsilon}')\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                self.add_log_data(f'Best rewards so far: {best_rewards}')\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_{i}'.format(self.path, i))\n",
    "                policy_dqn.save(f'{self.path}/mc_policy_best'.format(self.path))\n",
    "                \n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size and goal_reached:\n",
    "                \n",
    "                #print(f'OPTIMIZE Episode {i} Epsilon {epsilon} rewards {rewards}') # print(rewards)\n",
    "\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = epsilon = max(epsilon - 1/episodes, 0.01) # max(epsilon*0.99996, 0.05)#\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn = policy_dqn.deep_copy()\n",
    "                    step_count = 0\n",
    "                   \n",
    "                \n",
    "                \n",
    "        env.close()\n",
    "        policy_dqn.save(f'{self.path}/mc_policy_last_{self.tag}'.format(self.path, self.tag))\n",
    "        if (best_rewards == -200):\n",
    "            policy_dqn.save(f'{self.path}/mc_policy_best'.format(self.path))\n",
    "        self.save_reward_data(rewards_per_episode)\n",
    "        #print(rewards_per_episode)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        target_q_list = []\n",
    "        input_list = []\n",
    "        \n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor_g * target_dqn.forward(self.state_to_dqn_input(new_state)).max()\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            state_dsc = np.asarray(self.state_to_dqn_input(state))\n",
    "            input_list.append(state_dsc)\n",
    "            \n",
    "            target_q = target_dqn.forward(state_dsc)\n",
    "            \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target            \n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        #BACKPOP AND UPDATE on minibatch\n",
    "        x = np.asarray(input_list)\n",
    "        x = x[:, :, 0]\n",
    "        x = x.T\n",
    "\n",
    "        y = np.asarray(target_q_list)\n",
    "        y = y[:, :, 0]\n",
    "        y = y.T\n",
    "\n",
    "        policy_dqn.learn_one(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_dqn_input(self, state):\n",
    "        #state_p = np.digitize(state[0], self.pos_space)\n",
    "        #state_v = np.digitize(state[1], self.vel_space)\n",
    "        \n",
    "        # state_p = state[0]\n",
    "        # state_v = state[1]\n",
    "\n",
    "        return np.asarray([[state[0]], [state[1]], [state[2]], [state[3]]])\n",
    "\n",
    "        \n",
    "\n",
    "    def test(self, policy_dqn, episodes, render = False):\n",
    "        env = gym.make(self.game_name, render_mode='human' if render else None)\n",
    "        \n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        #self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        #self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        done_count = 0\n",
    "        medium_reward = 0\n",
    "        reward_list = []\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            state, info = env.reset()  # Initialize to state 0\n",
    "            Termination = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "            rewards = 0\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            \n",
    "            while(not Termination and not truncated):\n",
    "            #while(not done and not truncated):\n",
    "                state = self.state_to_dqn_input(state)\n",
    "                #print('state shape', state.shape)\n",
    "               \n",
    "                res = policy_dqn.forward(state)\n",
    "\n",
    "                action = res.argmax().item()\n",
    "                \n",
    "#Termination: Pole Angle is greater than ±12°\n",
    "#Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "#Truncation: Episode length is greater than 500 (200 for v0)\n",
    " \n",
    "                \n",
    "                state, reward, Termination, truncated, _ = env.step(action)\n",
    "                rewards+=reward\n",
    "                if (truncated):\n",
    "                    done_count += 1\n",
    "                    break\n",
    "\n",
    "            medium_reward += rewards\n",
    "            reward_list.append(rewards)\n",
    "                \n",
    "\n",
    "        \n",
    "        env.close()\n",
    "        medium_reward = medium_reward / episodes\n",
    "        return done_count*100.0/episodes, medium_reward, reward_list\n",
    "        \n",
    "    def save_info(s, ct, topology):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\")   \n",
    "       \n",
    "\n",
    "\n",
    "        f.write(\"data {}\\n\".format(s.ct))\n",
    "        f.write(\"tag {}\\n\".format(s.tag))\n",
    "        f.write(\"{}\\n\".format(s.game_name))\n",
    "        f.write(\"{}\\n\".format(s.discount_factor_g))\n",
    "        f.write(\"{}\\n\".format(s.mini_batch_size))\n",
    "        f.write(\"{}\\n\".format(s.num_divisions))\n",
    "        f.write(\"{}\\n\".format(s.replay_memory_size))\n",
    "        f.write(\"{}\\n\".format(s.network_sync_rate))\n",
    "        f.write(\"{}\\n\".format(topology))\n",
    "                         \n",
    "        f.close()\n",
    "    \n",
    "    def add_log_data(s, data):\n",
    "        file_path = f'{s.path}/info_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n",
    "        \n",
    "    def save_reward_data(s, data):\n",
    "        file_path = f'{s.path}/rewards_{s.tag}.txt'.format(s.path, s.tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"{}\\n\".format(data))\n",
    "                         \n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fcb1c73-969b-412d-9f70-af3ec6db3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basil\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "game_name = \"CartPole-v0\"\n",
    "\n",
    "sizes_list = [16, 12]\n",
    "learning_rate_a = 0.001\n",
    "\n",
    "env = gym.make(game_name)\n",
    "in_states = env.observation_space.shape[0]\n",
    "out_actions = env.action_space.n\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(in_states)\n",
    "print(out_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef80f4e5-b8ad-45b0-b2d7-2c9e0f84dccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16_16_00\n",
      "Best rewards so far: 12.0\n",
      "Best rewards so far: 19.0\n",
      "Best rewards so far: 21.0\n",
      "Best rewards so far: 25.0\n",
      "Best rewards so far: 32.0\n",
      "Best rewards so far: 34.0\n",
      "Best rewards so far: 37.0\n",
      "Best rewards so far: 48.0\n",
      "Best rewards so far: 62.0\n",
      "Best rewards so far: 68.0\n",
      "Best rewards so far: 77.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Best rewards so far: 86.0\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Best rewards so far: 91.0\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 92.0\n",
      "Best rewards so far: 161.0\n",
      "Best rewards so far: 225.0\n",
      "Best rewards so far: 238.0\n",
      "Best rewards so far: 277.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_01\n",
      "Best rewards so far: 33.0\n",
      "Best rewards so far: 51.0\n",
      "Best rewards so far: 70.0\n",
      "Best rewards so far: 77.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Best rewards so far: 85.0\n",
      "Best rewards so far: 107.0\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_02\n",
      "Best rewards so far: 20.0\n",
      "Best rewards so far: 21.0\n",
      "Best rewards so far: 25.0\n",
      "Best rewards so far: 42.0\n",
      "Best rewards so far: 68.0\n",
      "Best rewards so far: 99.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Best rewards so far: 101.0\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Best rewards so far: 117.0\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 234.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_03\n",
      "Best rewards so far: 17.0\n",
      "Best rewards so far: 21.0\n",
      "Best rewards so far: 37.0\n",
      "Best rewards so far: 45.0\n",
      "Best rewards so far: 88.0\n",
      "Best rewards so far: 122.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 164.0\n",
      "Best rewards so far: 227.0\n",
      "Best rewards so far: 290.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_04\n",
      "Best rewards so far: 19.0\n",
      "Best rewards so far: 29.0\n",
      "Best rewards so far: 35.0\n",
      "Best rewards so far: 81.0\n",
      "Best rewards so far: 86.0\n",
      "Best rewards so far: 87.0\n",
      "Best rewards so far: 89.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Best rewards so far: 100.0\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_05\n",
      "Best rewards so far: 16.0\n",
      "Best rewards so far: 31.0\n",
      "Best rewards so far: 49.0\n",
      "Best rewards so far: 104.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Best rewards so far: 112.0\n",
      "Best rewards so far: 178.0\n",
      "Best rewards so far: 222.0\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Best rewards so far: 300.0\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_06\n",
      "Best rewards so far: 37.0\n",
      "Best rewards so far: 38.0\n",
      "Best rewards so far: 40.0\n",
      "Best rewards so far: 67.0\n",
      "Best rewards so far: 73.0\n",
      "Best rewards so far: 79.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Best rewards so far: 85.0\n",
      "Best rewards so far: 100.0\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_07\n",
      "Best rewards so far: 11.0\n",
      "Best rewards so far: 33.0\n",
      "Best rewards so far: 36.0\n",
      "Best rewards so far: 40.0\n",
      "Best rewards so far: 58.0\n",
      "Best rewards so far: 62.0\n",
      "Best rewards so far: 73.0\n",
      "Best rewards so far: 94.0\n",
      "Best rewards so far: 119.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Best rewards so far: 122.0\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Best rewards so far: 132.0\n",
      "Best rewards so far: 142.0\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 200.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_08\n",
      "Best rewards so far: 20.0\n",
      "Best rewards so far: 27.0\n",
      "Best rewards so far: 63.0\n",
      "Best rewards so far: 80.0\n",
      "Best rewards so far: 81.0\n",
      "Best rewards so far: 112.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_09\n",
      "Best rewards so far: 15.0\n",
      "Best rewards so far: 24.0\n",
      "Best rewards so far: 43.0\n",
      "Best rewards so far: 48.0\n",
      "Best rewards so far: 52.0\n",
      "Best rewards so far: 90.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 105.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_10\n",
      "Best rewards so far: 25.0\n",
      "Best rewards so far: 46.0\n",
      "Best rewards so far: 84.0\n",
      "Best rewards so far: 87.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Best rewards so far: 96.0\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Best rewards so far: 105.0\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Best rewards so far: 219.0\n",
      "Best rewards so far: 233.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_11\n",
      "Best rewards so far: 14.0\n",
      "Best rewards so far: 28.0\n",
      "Best rewards so far: 33.0\n",
      "Best rewards so far: 39.0\n",
      "Best rewards so far: 41.0\n",
      "Best rewards so far: 68.0\n",
      "Best rewards so far: 86.0\n",
      "Best rewards so far: 100.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Best rewards so far: 138.0\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_12\n",
      "Best rewards so far: 25.0\n",
      "Best rewards so far: 36.0\n",
      "Best rewards so far: 48.0\n",
      "Best rewards so far: 61.0\n",
      "Best rewards so far: 79.0\n",
      "Best rewards so far: 81.0\n",
      "Best rewards so far: 91.0\n",
      "Best rewards so far: 93.0\n",
      "Best rewards so far: 109.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 113.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Best rewards so far: 122.0\n",
      "Best rewards so far: 125.0\n",
      "Best rewards so far: 167.0\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_13\n",
      "Best rewards so far: 52.0\n",
      "Best rewards so far: 59.0\n",
      "Best rewards so far: 60.0\n",
      "Best rewards so far: 77.0\n",
      "Best rewards so far: 98.0\n",
      "Episode 1000 Epsilon 0.900100000000011\n",
      "Episode 2000 Epsilon 0.800100000000022\n",
      "Episode 3000 Epsilon 0.700100000000033\n",
      "Episode 4000 Epsilon 0.600100000000044\n",
      "Best rewards so far: 105.0\n",
      "Episode 5000 Epsilon 0.5001000000000551\n",
      "Best rewards so far: 107.0\n",
      "Episode 6000 Epsilon 0.40010000000006607\n",
      "Episode 7000 Epsilon 0.3001000000000771\n",
      "Episode 8000 Epsilon 0.2001000000000881\n",
      "Episode 9000 Epsilon 0.10010000000009565\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_14\n",
      "Best rewards so far: 38.0\n",
      "Best rewards so far: 39.0\n",
      "Best rewards so far: 54.0\n",
      "Best rewards so far: 71.0\n",
      "Best rewards so far: 72.0\n",
      "Best rewards so far: 104.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "16_16_15\n",
      "Best rewards so far: 37.0\n",
      "Best rewards so far: 42.0\n",
      "Best rewards so far: 54.0\n",
      "Best rewards so far: 61.0\n",
      "Best rewards so far: 87.0\n",
      "Best rewards so far: 94.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_00\n",
      "Best rewards so far: 11.0\n",
      "Best rewards so far: 23.0\n",
      "Best rewards so far: 27.0\n",
      "Best rewards so far: 39.0\n",
      "Best rewards so far: 46.0\n",
      "Best rewards so far: 97.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Best rewards so far: 124.0\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_01\n",
      "Best rewards so far: 17.0\n",
      "Best rewards so far: 23.0\n",
      "Best rewards so far: 26.0\n",
      "Best rewards so far: 33.0\n",
      "Best rewards so far: 37.0\n",
      "Best rewards so far: 56.0\n",
      "Best rewards so far: 59.0\n",
      "Best rewards so far: 73.0\n",
      "Best rewards so far: 76.0\n",
      "Best rewards so far: 87.0\n",
      "Best rewards so far: 118.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Best rewards so far: 126.0\n",
      "Best rewards so far: 150.0\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Best rewards so far: 152.0\n",
      "Best rewards so far: 173.0\n",
      "Best rewards so far: 192.0\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 300.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_02\n",
      "Best rewards so far: 18.0\n",
      "Best rewards so far: 24.0\n",
      "Best rewards so far: 25.0\n",
      "Best rewards so far: 29.0\n",
      "Best rewards so far: 34.0\n",
      "Best rewards so far: 39.0\n",
      "Best rewards so far: 41.0\n",
      "Best rewards so far: 71.0\n",
      "Best rewards so far: 75.0\n",
      "Best rewards so far: 82.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Best rewards so far: 83.0\n",
      "Best rewards so far: 87.0\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Best rewards so far: 102.0\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Best rewards so far: 108.0\n",
      "Best rewards so far: 125.0\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Best rewards so far: 300.0\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_03\n",
      "Best rewards so far: 16.0\n",
      "Best rewards so far: 42.0\n",
      "Best rewards so far: 60.0\n",
      "Best rewards so far: 97.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Best rewards so far: 98.0\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Best rewards so far: 111.0\n",
      "Best rewards so far: 122.0\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Best rewards so far: 209.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_04\n",
      "Best rewards so far: 15.0\n",
      "Best rewards so far: 86.0\n",
      "Best rewards so far: 89.0\n",
      "Episode 1000 Epsilon 0.900100000000011\n",
      "Episode 2000 Epsilon 0.800100000000022\n",
      "Episode 3000 Epsilon 0.700100000000033\n",
      "Episode 4000 Epsilon 0.600100000000044\n",
      "Episode 5000 Epsilon 0.5001000000000551\n",
      "Episode 6000 Epsilon 0.40010000000006607\n",
      "Episode 7000 Epsilon 0.3001000000000771\n",
      "Best rewards so far: 150.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 8000 Epsilon 0.2001000000000881\n",
      "Episode 9000 Epsilon 0.10010000000009565\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_05\n",
      "Best rewards so far: 16.0\n",
      "Best rewards so far: 27.0\n",
      "Best rewards so far: 31.0\n",
      "Best rewards so far: 60.0\n",
      "Best rewards so far: 64.0\n",
      "Best rewards so far: 71.0\n",
      "Best rewards so far: 93.0\n",
      "Best rewards so far: 113.0\n",
      "Best rewards so far: 137.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Best rewards so far: 300.0\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_06\n",
      "Best rewards so far: 15.0\n",
      "Best rewards so far: 18.0\n",
      "Best rewards so far: 39.0\n",
      "Best rewards so far: 44.0\n",
      "Best rewards so far: 67.0\n",
      "Best rewards so far: 76.0\n",
      "Best rewards so far: 99.0\n",
      "Episode 1000 Epsilon 0.900400000000011\n",
      "Best rewards so far: 112.0\n",
      "Episode 2000 Epsilon 0.800400000000022\n",
      "Episode 3000 Epsilon 0.700400000000033\n",
      "Episode 4000 Epsilon 0.600400000000044\n",
      "Episode 5000 Epsilon 0.500400000000055\n",
      "Episode 6000 Epsilon 0.40040000000006604\n",
      "Episode 7000 Epsilon 0.30040000000007705\n",
      "Episode 8000 Epsilon 0.20040000000008806\n",
      "Episode 9000 Epsilon 0.10040000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_07\n",
      "Best rewards so far: 46.0\n",
      "Best rewards so far: 64.0\n",
      "Best rewards so far: 75.0\n",
      "Episode 1000 Epsilon 0.900100000000011\n",
      "Best rewards so far: 81.0\n",
      "Episode 2000 Epsilon 0.800100000000022\n",
      "Best rewards so far: 91.0\n",
      "Episode 3000 Epsilon 0.700100000000033\n",
      "Episode 4000 Epsilon 0.600100000000044\n",
      "Episode 5000 Epsilon 0.5001000000000551\n",
      "Best rewards so far: 96.0\n",
      "Episode 6000 Epsilon 0.40010000000006607\n",
      "Best rewards so far: 111.0\n",
      "Best rewards so far: 120.0\n",
      "Episode 7000 Epsilon 0.3001000000000771\n",
      "Episode 8000 Epsilon 0.2001000000000881\n",
      "Best rewards so far: 126.0\n",
      "Episode 9000 Epsilon 0.10010000000009565\n",
      "Best rewards so far: 139.0\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_08\n",
      "Best rewards so far: 37.0\n",
      "Best rewards so far: 99.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Best rewards so far: 101.0\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_09\n",
      "Best rewards so far: 44.0\n",
      "Best rewards so far: 49.0\n",
      "Best rewards so far: 67.0\n",
      "Best rewards so far: 81.0\n",
      "Best rewards so far: 87.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Best rewards so far: 193.0\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_10\n",
      "Best rewards so far: 15.0\n",
      "Best rewards so far: 25.0\n",
      "Best rewards so far: 26.0\n",
      "Best rewards so far: 42.0\n",
      "Best rewards so far: 45.0\n",
      "Best rewards so far: 51.0\n",
      "Best rewards so far: 69.0\n",
      "Best rewards so far: 70.0\n",
      "Best rewards so far: 74.0\n",
      "Best rewards so far: 78.0\n",
      "Best rewards so far: 91.0\n",
      "Episode 1000 Epsilon 0.900300000000011\n",
      "Episode 2000 Epsilon 0.800300000000022\n",
      "Episode 3000 Epsilon 0.700300000000033\n",
      "Episode 4000 Epsilon 0.600300000000044\n",
      "Best rewards so far: 100.0\n",
      "Best rewards so far: 127.0\n",
      "Episode 5000 Epsilon 0.500300000000055\n",
      "Best rewards so far: 149.0\n",
      "Best rewards so far: 173.0\n",
      "Best rewards so far: 187.0\n",
      "Best rewards so far: 231.0\n",
      "Best rewards so far: 262.0\n",
      "Best rewards so far: 300.0\n",
      "Episode 6000 Epsilon 0.40030000000006605\n",
      "Episode 7000 Epsilon 0.30030000000007706\n",
      "Episode 8000 Epsilon 0.20030000000008807\n",
      "Episode 9000 Epsilon 0.10030000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_11\n",
      "Best rewards so far: 15.0\n",
      "Best rewards so far: 33.0\n",
      "Best rewards so far: 44.0\n",
      "Best rewards so far: 63.0\n",
      "Best rewards so far: 73.0\n",
      "Best rewards so far: 80.0\n",
      "Best rewards so far: 88.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_12\n",
      "Best rewards so far: 12.0\n",
      "Best rewards so far: 17.0\n",
      "Best rewards so far: 24.0\n",
      "Best rewards so far: 28.0\n",
      "Best rewards so far: 45.0\n",
      "Best rewards so far: 51.0\n",
      "Best rewards so far: 66.0\n",
      "Best rewards so far: 68.0\n",
      "Best rewards so far: 79.0\n",
      "Best rewards so far: 87.0\n",
      "Episode 1000 Epsilon 0.900500000000011\n",
      "Episode 2000 Epsilon 0.800500000000022\n",
      "Episode 3000 Epsilon 0.700500000000033\n",
      "Episode 4000 Epsilon 0.600500000000044\n",
      "Episode 5000 Epsilon 0.500500000000055\n",
      "Episode 6000 Epsilon 0.400500000000066\n",
      "Episode 7000 Epsilon 0.30050000000007704\n",
      "Episode 8000 Epsilon 0.20050000000008805\n",
      "Episode 9000 Epsilon 0.10050000000009567\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_13\n",
      "Best rewards so far: 13.0\n",
      "Best rewards so far: 53.0\n",
      "Best rewards so far: 55.0\n",
      "Best rewards so far: 63.0\n",
      "Best rewards so far: 84.0\n",
      "Best rewards so far: 92.0\n",
      "Episode 1000 Epsilon 0.900100000000011\n",
      "Best rewards so far: 102.0\n",
      "Episode 2000 Epsilon 0.800100000000022\n",
      "Episode 3000 Epsilon 0.700100000000033\n",
      "Episode 4000 Epsilon 0.600100000000044\n",
      "Episode 5000 Epsilon 0.5001000000000551\n",
      "Episode 6000 Epsilon 0.40010000000006607\n",
      "Episode 7000 Epsilon 0.3001000000000771\n",
      "Episode 8000 Epsilon 0.2001000000000881\n",
      "Episode 9000 Epsilon 0.10010000000009565\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_14\n",
      "Best rewards so far: 25.0\n",
      "Best rewards so far: 35.0\n",
      "Best rewards so far: 38.0\n",
      "Best rewards so far: 45.0\n",
      "Best rewards so far: 51.0\n",
      "Best rewards so far: 82.0\n",
      "Best rewards so far: 96.0\n",
      "Best rewards so far: 104.0\n",
      "Episode 1000 Epsilon 0.900200000000011\n",
      "Episode 2000 Epsilon 0.800200000000022\n",
      "Episode 3000 Epsilon 0.700200000000033\n",
      "Episode 4000 Epsilon 0.600200000000044\n",
      "Episode 5000 Epsilon 0.500200000000055\n",
      "Episode 6000 Epsilon 0.40020000000006606\n",
      "Episode 7000 Epsilon 0.30020000000007707\n",
      "Episode 8000 Epsilon 0.20020000000008809\n",
      "Episode 9000 Epsilon 0.10020000000009566\n",
      "Episode 10000 Epsilon 0.01\n",
      "12_12_15\n",
      "Best rewards so far: 41.0\n",
      "Best rewards so far: 64.0\n",
      "Best rewards so far: 78.0\n",
      "Best rewards so far: 90.0\n",
      "Best rewards so far: 119.0\n",
      "Episode 1000 Epsilon 0.900100000000011\n",
      "Episode 2000 Epsilon 0.800100000000022\n",
      "Episode 3000 Epsilon 0.700100000000033\n",
      "Episode 4000 Epsilon 0.600100000000044\n",
      "Episode 5000 Epsilon 0.5001000000000551\n",
      "Episode 6000 Epsilon 0.40010000000006607\n",
      "Episode 7000 Epsilon 0.3001000000000771\n",
      "Episode 8000 Epsilon 0.2001000000000881\n",
      "Episode 9000 Epsilon 0.10010000000009565\n",
      "Best rewards so far: 130.0\n",
      "Best rewards so far: 132.0\n",
      "Episode 10000 Epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "#sizes_list = [4, 8, 12, 16, 20, 24, 28, 32, 36]\n",
    "\n",
    "\n",
    "\n",
    "a_type1 = 'ReLU'\n",
    "a_type2 = 'Linear'\n",
    "\n",
    "ct = datetime.datetime.now()\n",
    "ct = str(ct)\n",
    "ct = ct.replace(\":\", \"-\")\n",
    "ct = ct.replace(\" \", \"_\")\n",
    "ct = ct[:-7]\n",
    "\n",
    "model_per_var = 4\n",
    "n_of_var = 4\n",
    "n_models = model_per_var * n_of_var\n",
    "\n",
    "n_tests_for_model = 100\n",
    "epochs = 10000\n",
    "\n",
    "\n",
    "for each in sizes_list:\n",
    "    for i in range(n_models):\n",
    "        # if (i < 5):\n",
    "        #     learning_rate_a = 0.001 \n",
    "        # elif (i >= 10):# and i < 20):\n",
    "        #     learning_rate_a = 0.00005\n",
    "\n",
    "        if (i < model_per_var * 1):\n",
    "            learning_rate_a = 0.2\n",
    "        elif (i >= model_per_var * 1 and i < model_per_var * 2):\n",
    "            learning_rate_a = 0.1 \n",
    "        elif (i >= model_per_var * 2 and i < model_per_var * 3):\n",
    "            learning_rate_a = 0.01\n",
    "        elif(i>=model_per_var * 3 and i < model_per_var * 4):\n",
    "            learning_rate_a = 0.001 \n",
    "\n",
    "        tag = \"{:02d}_{:02d}_{:02d}\".format(each, each, i)\n",
    "        print(tag)\n",
    "        \n",
    "        h1_nodes = each\n",
    "        h2_nodes = each\n",
    "\n",
    "        \n",
    "        layers_net = [input_layer(in_states), \n",
    "        layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type=a_type1), \n",
    "        layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type=a_type1), \n",
    "        layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type=a_type2)]\n",
    "        \n",
    "        policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "        policy_dqn.append_layers(layers_net)\n",
    "        \n",
    "        target_dqn = plastic_nn()\n",
    "        target_dqn = policy_dqn.deep_copy()\n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "        \n",
    "        mountaincar = DQN(ct, tag = tag, game_name = game_name, \n",
    "                          discount_factor_g = 0.9, mini_batch_size = 64, replay_memory_size = 100000, network_sync_rate = 50000)\n",
    "        \n",
    "        mountaincar.save_info(ct, f'lr: {learning_rate_a} \\nin:{in_states} \\nh:{h1_nodes}x2 (--) \\nout:{out_actions} \\na1:{a_type1} \\na2:{a_type2} \\n'.format(\n",
    "        learning_rate_a, in_states, h1_nodes, out_actions, a_type1, a_type2))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        mountaincar.train(policy_dqn, target_dqn, epochs, False)\n",
    "        \n",
    "        folder_path = ct+'/'+tag\n",
    "        policy_dqn.load(f'{folder_path}/mc_policy_best')\n",
    "        test_res, medium_reward, reward_list = mountaincar.test(policy_dqn, n_tests_for_model, render = False)\n",
    "\n",
    "        file_path = '{}/tests_{}.txt'.format(folder_path, tag)\n",
    "        f = open(file_path, \"a\") \n",
    "        f.write(\"n_tests {}\\n\".format(n_tests_for_model))\n",
    "        f.write(\"done % {}\\n\".format(test_res))\n",
    "        f.write(\"medium_reward {}\\n\".format(medium_reward))   \n",
    "        f.write(\"reward_list {}\\n\".format(reward_list))   \n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        del policy_dqn\n",
    "        del target_dqn\n",
    "        del mountaincar\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f045c962-6024-43b8-8527-1376ce918872",
   "metadata": {
    "id": "f045c962-6024-43b8-8527-1376ce918872",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learning_rate_a = 0.0001\n",
    "# in_states = 2\n",
    "# h1_nodes = 2\n",
    "# h2_nodes = 2\n",
    "# out_actions = 3\n",
    "\n",
    "# a_type1 = 'ReLU'\n",
    "# a_type2 = 'Linear'\n",
    "\n",
    "# layers_net = [input_layer(in_states), \n",
    "# layer(lr = learning_rate_a, prev_size = in_states, my_size=h1_nodes, activation_type=a_type1), \n",
    "# layer(lr = learning_rate_a, prev_size = h1_nodes, my_size=h2_nodes, activation_type=a_type1), \n",
    "# layer(lr = learning_rate_a, prev_size = h2_nodes, my_size=out_actions, activation_type=a_type2)]\n",
    "\n",
    "# policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "# policy_dqn.append_layers(layers_net)\n",
    "\n",
    "# target_dqn = plastic_nn()\n",
    "# target_dqn = policy_dqn.deep_copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ee7d52-ac65-4452-a85e-99e32613d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dqn.print_info()\n",
    "# target_dqn.add_neuron(1)\n",
    "# target_dqn.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c8eaab-4e8c-497b-9cfc-0dd2cf1f9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = datetime.datetime.now()\n",
    "# ct = str(ct)\n",
    "# ct = ct.replace(\":\", \"-\")\n",
    "# ct = ct.replace(\" \", \"_\")\n",
    "# ct = ct[:-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219350b2-41b7-44de-b105-8ab7aefd85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mountaincar = DQN(ct, path = ct, game_name = 'MountainCar-v0', discount_factor_g = 0.9, mini_batch_size = 64, \n",
    "#                   num_divisions = 50, replay_memory_size = 100000, network_sync_rate = 50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
    "outputId": "5d51151e-415f-4612-c064-a8a6a337d5d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mountaincar.save_info(ct, f'lr: {learning_rate_a} \\nin:{in_states} \\nh:{h1_nodes}x2 (--) \\nout:{out_actions} \\na1:{a_type1} \\na2:{a_type2} \\n'.format(\n",
    "#     learning_rate_a, in_states, h1_nodes, out_actions, a_type1, a_type2))\n",
    "# mountaincar.train(policy_dqn, target_dqn, 25000, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fce1ba9-219f-4f4f-89f3-0b53692ef62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myADAM_andENV/31_05/2024-05-31_12-45-39/mc_policy_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7445aaf2-91ce-4198-9f2c-d0f53f334af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_dqn.load(f'{ct}/mc_policy_24204') #f'{ct}/mc_policy_21560'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29a07137-0867-4bfd-a11e-bc208a3cee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = datetime.datetime.now()\n",
    "# ct = str(ct)\n",
    "# ct = ct.replace(\":\", \"-\")\n",
    "# ct = ct.replace(\" \", \"_\")\n",
    "# ct = ct[:-7]\n",
    "\n",
    "# mountaincar = DQN(ct, path = ct, game_name = game_name, discount_factor_g = 0.9, mini_batch_size = 64, \n",
    "#                    num_divisions = 50, replay_memory_size = 100000, network_sync_rate = 50000)\n",
    "# policy_dqn = plastic_nn(optimizer=\"Adam\")\n",
    "# policy_dqn.load('2024-06-02_23-54-14/08_08_00/mc_policy_best') #f'{ct}/mc_policy_21560'\n",
    "\n",
    "#mountaincar.test(policy_dqn, 3, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
   "metadata": {
    "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2baf01c5-f2fc-48c3-aee3-27e1ff536917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#policy_dqn.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ea8963d-2e57-46f6-b2ba-35c481ea6423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#policy_dqn.print_info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c78fedb0-800f-495b-b18b-2e3fa0acc238",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
