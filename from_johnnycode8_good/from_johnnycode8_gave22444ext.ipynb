{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a340e3ad-4a53-4a63-ac93-a62e13808ab4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a340e3ad-4a53-4a63-ac93-a62e13808ab4",
    "outputId": "17f912b7-f94b-4efe-ff93-a3059d1e32ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from jinja2->torch==2.2.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from sympy->torch==2.2.1) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "K5feZBXDqSyO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5feZBXDqSyO",
    "outputId": "f8fa2bd4-7df7-40c6-97a6-3a71739035a2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (7.1.0)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (2.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290e9d45-e28b-4603-9058-3260849a1668",
   "metadata": {
    "id": "290e9d45-e28b-4603-9058-3260849a1668"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99242b2c-e34a-4e2b-a0ce-69dc353240c2",
   "metadata": {
    "id": "99242b2c-e34a-4e2b-a0ce-69dc353240c2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        #print(x)\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# MountainCar Deep Q-Learning\n",
    "class MountainCarDQL():\n",
    "    # Hyperparameters (adjustable)\n",
    "    learning_rate_a = 0.01         # learning rate (alpha)\n",
    "    discount_factor_g = 0.9         # discount rate (gamma)\n",
    "    network_sync_rate = 50000          # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 100000       # size of replay memory\n",
    "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "    num_divisions = 20\n",
    "\n",
    "    # Neural Network\n",
    "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "\n",
    "    # Train the environment\n",
    "    def train(self, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count=0\n",
    "        goal_reached=False\n",
    "        best_rewards=-200\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "\n",
    "            rewards = 0\n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and rewards>-1000):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    # select best action\n",
    "                    with torch.no_grad():\n",
    "                        #print(policy_dqn(self.state_to_dqn_input(state)))\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "                # Accumulate reward\n",
    "                rewards += reward\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count+=1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            rewards_per_episode.append(rewards)\n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                # Save policy\n",
    "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size:# and goal_reached:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count=0\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
    "        # Create new graph\n",
    "        plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
    "        # for x in range(len(rewards_per_episode)):\n",
    "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        # plt.plot(sum_rewards)\n",
    "        plt.plot(rewards_per_episode)\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        # Save plots\n",
    "        plt.savefig('mountaincar_dql.png')\n",
    "    # Optimize policy network\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated:\n",
    "                # Agent receive reward of 0 for reaching goal.\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value\n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "        #print(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    '''\n",
    "    Converts a state (position, velocity) to tensor representation.\n",
    "    Example:\n",
    "    Input = (0.3, -0.03)\n",
    "    Return = tensor([16, 6])\n",
    "    '''\n",
    "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "\n",
    "        return torch.FloatTensor([state_p, state_v])\n",
    "\n",
    "    # Run the environment with the learned policy\n",
    "    def test(self, episodes, model_filepath):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "                # Select best action\n",
    "                with torch.no_grad():\n",
    "                    var = policy_dqn(self.state_to_dqn_input(state))\n",
    "                    print(var)\n",
    "                    action = var.argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "     # Run the environment with the learned policy\n",
    "    def print(self, model_filepath):\n",
    "       \n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=2, h1_nodes=10, out_actions=3)\n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        print(policy_dqn)\n",
    "        print(policy_dqn.fc1.weight, policy_dqn.fc1.bias) # = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        print(policy_dqn.out.weight, policy_dqn.out.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f045c962-6024-43b8-8527-1376ce918872",
   "metadata": {
    "id": "f045c962-6024-43b8-8527-1376ce918872",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mountaincar = MountainCarDQL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
    "outputId": "5d51151e-415f-4612-c064-a8a6a337d5d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mountaincar.train(25000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
   "metadata": {
    "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.7866, -9.7891, -9.8851])\n",
      "tensor([-9.7866, -9.7891, -9.8851])\n",
      "tensor([-9.7866, -9.7891, -9.8851])\n",
      "tensor([-9.7866, -9.7891, -9.8851])\n",
      "tensor([-9.7866, -9.7891, -9.8851])\n",
      "tensor([-9.7866, -9.7891, -9.8851])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.8695,  -9.8759, -10.0394])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([ -9.9546,  -9.9623, -10.1413])\n",
      "tensor([-10.0398, -10.0487, -10.2432])\n",
      "tensor([-10.0398, -10.0487, -10.2432])\n",
      "tensor([-10.0398, -10.0487, -10.2432])\n",
      "tensor([-10.0398, -10.0487, -10.2432])\n",
      "tensor([-10.0398, -10.0487, -10.2432])\n",
      "tensor([-10.0398, -10.0487, -10.2432])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([ -9.9569,  -9.9619, -10.0889])\n",
      "tensor([-9.8740, -9.8751, -9.9346])\n",
      "tensor([-9.8740, -9.8751, -9.9346])\n",
      "tensor([-9.8740, -9.8751, -9.9346])\n",
      "tensor([-9.8740, -9.8751, -9.9346])\n",
      "tensor([-9.7889, -9.7887, -9.8327])\n",
      "tensor([-9.7889, -9.7887, -9.8327])\n",
      "tensor([-9.7889, -9.7887, -9.8327])\n",
      "tensor([-9.7889, -9.7887, -9.8327])\n",
      "tensor([-9.7060, -9.7020, -9.6784])\n",
      "tensor([-9.7060, -9.7020, -9.6784])\n",
      "tensor([-9.7060, -9.7020, -9.6784])\n",
      "tensor([-9.7060, -9.7020, -9.6784])\n",
      "tensor([-9.5380, -9.5288, -9.4221])\n",
      "tensor([-9.5380, -9.5288, -9.4221])\n",
      "tensor([-9.5380, -9.5288, -9.4221])\n",
      "tensor([-9.5380, -9.5288, -9.4221])\n",
      "tensor([-9.4528, -9.4424, -9.3202])\n",
      "tensor([-9.3699, -9.3557, -9.1659])\n",
      "tensor([-9.3699, -9.3557, -9.1659])\n",
      "tensor([-9.3699, -9.3557, -9.1659])\n",
      "tensor([-9.2848, -9.2693, -9.0640])\n",
      "tensor([-9.2848, -9.2693, -9.0640])\n",
      "tensor([-9.2848, -9.2693, -9.0640])\n",
      "tensor([-9.1997, -9.1829, -8.9620])\n",
      "tensor([-9.1997, -9.1829, -8.9620])\n",
      "tensor([-9.1997, -9.1829, -8.9620])\n",
      "tensor([-9.1145, -9.0965, -8.8601])\n",
      "tensor([-9.1974, -9.1832, -9.0144])\n",
      "tensor([-9.1974, -9.1832, -9.0144])\n",
      "tensor([-9.1974, -9.1832, -9.0144])\n",
      "tensor([-9.1123, -9.0968, -8.9125])\n",
      "tensor([-9.1123, -9.0968, -8.9125])\n",
      "tensor([-9.1123, -9.0968, -8.9125])\n",
      "tensor([-9.1123, -9.0968, -8.9125])\n",
      "tensor([-9.1951, -9.1836, -9.0668])\n",
      "tensor([-9.1100, -9.0972, -8.9649])\n",
      "tensor([-9.1100, -9.0972, -8.9649])\n",
      "tensor([-9.1100, -9.0972, -8.9649])\n",
      "tensor([-9.1100, -9.0972, -8.9649])\n",
      "tensor([-9.1929, -9.1840, -9.1192])\n",
      "tensor([-9.1929, -9.1840, -9.1192])\n",
      "tensor([-9.1929, -9.1840, -9.1192])\n",
      "tensor([-9.1929, -9.1840, -9.1192])\n",
      "tensor([-9.1929, -9.1840, -9.1192])\n",
      "tensor([-9.2757, -9.2707, -9.2735])\n",
      "tensor([-9.2757, -9.2707, -9.2735])\n",
      "tensor([-9.2757, -9.2707, -9.2735])\n",
      "tensor([-9.3586, -9.3575, -9.4278])\n",
      "tensor([-9.3586, -9.3575, -9.4278])\n",
      "tensor([-9.3586, -9.3575, -9.4278])\n",
      "tensor([-9.4415, -9.4443, -9.5821])\n",
      "tensor([-9.4415, -9.4443, -9.5821])\n",
      "tensor([-9.5244, -9.5310, -9.7364])\n",
      "tensor([-9.6095, -9.6174, -9.8384])\n",
      "tensor([-9.6924, -9.7042, -9.9927])\n",
      "tensor([-9.6924, -9.7042, -9.9927])\n",
      "tensor([ -9.7775,  -9.7906, -10.0946])\n",
      "tensor([ -9.8604,  -9.8773, -10.2489])\n",
      "tensor([ -9.8604,  -9.8773, -10.2489])\n",
      "tensor([ -9.6734,  -9.6906, -10.0881])\n",
      "tensor([ -9.6734,  -9.6906, -10.0881])\n",
      "tensor([ -9.6542,  -9.6712, -10.0675])\n",
      "tensor([-9.0808, -9.0928, -9.4509])\n",
      "tensor([-9.0616, -9.0734, -9.4302])\n",
      "tensor([-9.0616, -9.0734, -9.4302])\n",
      "tensor([-9.0424, -9.0540, -9.4095])\n",
      "tensor([-9.0231, -9.0346, -9.3888])\n",
      "tensor([-9.0231, -9.0346, -9.3888])\n",
      "tensor([-9.0039, -9.0152, -9.3681])\n",
      "tensor([-9.0039, -9.0152, -9.3681])\n",
      "tensor([-8.9846, -8.9958, -9.3475])\n",
      "tensor([-8.9846, -8.9958, -9.3475])\n",
      "tensor([-8.9654, -8.9763, -9.3268])\n",
      "tensor([-8.9654, -8.9763, -9.3268])\n",
      "tensor([-9.5195, -9.5354, -9.9226])\n",
      "tensor([-9.5195, -9.5354, -9.9226])\n",
      "tensor([-9.5003, -9.5160, -9.9020])\n",
      "tensor([-9.5003, -9.5160, -9.9020])\n",
      "tensor([-10.2975, -10.3075, -10.4966])\n",
      "tensor([-10.2975, -10.3075, -10.4966])\n",
      "tensor([-10.2975, -10.3075, -10.4966])\n",
      "tensor([-10.2146, -10.2207, -10.3423])\n",
      "tensor([-10.2146, -10.2207, -10.3423])\n",
      "tensor([-10.2146, -10.2207, -10.3423])\n",
      "tensor([-10.2146, -10.2207, -10.3423])\n",
      "tensor([-10.2146, -10.2207, -10.3423])\n",
      "tensor([-10.2146, -10.2207, -10.3423])\n",
      "tensor([-10.1317, -10.1340, -10.1880])\n",
      "tensor([-10.1317, -10.1340, -10.1880])\n",
      "tensor([-10.1317, -10.1340, -10.1880])\n",
      "tensor([-10.0466, -10.0476, -10.0861])\n",
      "tensor([-10.0466, -10.0476, -10.0861])\n",
      "tensor([-9.9637, -9.9608, -9.9318])\n",
      "tensor([-9.9637, -9.9608, -9.9318])\n",
      "tensor([-9.8808, -9.8740, -9.7774])\n",
      "tensor([-9.7957, -9.7876, -9.6755])\n",
      "tensor([-9.7957, -9.7876, -9.6755])\n",
      "tensor([-9.7128, -9.7009, -9.5212])\n",
      "tensor([-9.6276, -9.6145, -9.4193])\n",
      "tensor([-9.5448, -9.5277, -9.2650])\n",
      "tensor([-9.4596, -9.4413, -9.1630])\n",
      "tensor([-9.3767, -9.3546, -9.0087])\n",
      "tensor([-9.2916, -9.2682, -8.9068])\n",
      "tensor([-9.2916, -9.2682, -8.9068])\n",
      "tensor([-9.2065, -9.1818, -8.8049])\n",
      "tensor([-9.0384, -9.0086, -8.5486])\n",
      "tensor([-9.0384, -9.0086, -8.5486])\n",
      "tensor([-8.9533, -8.9222, -8.4467])\n",
      "tensor([-8.9533, -8.9222, -8.4467])\n",
      "tensor([-8.8681, -8.8358, -8.3448])\n",
      "tensor([-8.7830, -8.7494, -8.2428])\n",
      "tensor([-8.7830, -8.7494, -8.2428])\n",
      "tensor([-8.6979, -8.6630, -8.1409])\n",
      "tensor([-8.7807, -8.7498, -8.2952])\n",
      "tensor([-8.6956, -8.6634, -8.1933])\n",
      "tensor([-8.6105, -8.5770, -8.0914])\n",
      "tensor([-8.6105, -8.5770, -8.0914])\n",
      "tensor([-8.5253, -8.4906, -7.9895])\n",
      "tensor([-8.6082, -8.5774, -8.1438])\n",
      "tensor([-8.5230, -8.4910, -8.0418])\n",
      "tensor([-8.5230, -8.4910, -8.0418])\n",
      "tensor([-8.5230, -8.4910, -8.0418])\n",
      "tensor([-8.4379, -8.4046, -7.9399])\n",
      "tensor([-8.4379, -8.4046, -7.9399])\n",
      "tensor([-8.3528, -8.3182, -7.8380])\n",
      "tensor([-8.3528, -8.3182, -7.8380])\n"
     ]
    }
   ],
   "source": [
    "mountaincar.test(1, \"mountaincar_dql_23431.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de7f5e5-a80c-4f80-8342-2327a0de9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.1692, -0.5422],\n",
      "        [-0.5403, -0.1118],\n",
      "        [-0.7500, -0.0213],\n",
      "        [-0.7580,  0.0218],\n",
      "        [ 0.0900,  0.5659],\n",
      "        [ 0.0203,  0.6060],\n",
      "        [-0.2351, -0.3665],\n",
      "        [-0.0348, -0.7156],\n",
      "        [-0.1158, -0.5073],\n",
      "        [-0.5872,  0.0215]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.0757, -0.6626, -0.3756,  0.2973, -3.4696,  4.5481, -0.5467, -0.2575,\n",
      "         0.6185,  0.1723], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3058,  0.2379, -0.0113, -0.1135,  1.1597, -0.9462, -0.0374,  0.2266,\n",
      "         -0.2753, -0.2412],\n",
      "        [ 0.1222,  0.1303,  0.1516,  0.0678,  1.1755, -0.9546, -0.1078,  0.0779,\n",
      "         -0.0402,  0.0219],\n",
      "        [-0.2718,  0.3103,  0.1038,  0.0866,  1.3623, -1.0175, -0.0760,  0.2576,\n",
      "         -0.1153,  0.0337]], requires_grad=True) Parameter containing:\n",
      "tensor([-2.8841, -2.8413, -2.7873], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mountaincar.print(\"mountaincar_dql_23431.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4f8bd-20f0-4ca2-8cf5-06597b57b6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
