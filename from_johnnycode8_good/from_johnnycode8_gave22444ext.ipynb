{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a340e3ad-4a53-4a63-ac93-a62e13808ab4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a340e3ad-4a53-4a63-ac93-a62e13808ab4",
    "outputId": "17f912b7-f94b-4efe-ff93-a3059d1e32ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.2.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (3.13.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from torch==2.2.1) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from jinja2->torch==2.2.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from sympy->torch==2.2.1) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "K5feZBXDqSyO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5feZBXDqSyO",
    "outputId": "f8fa2bd4-7df7-40c6-97a6-3a71739035a2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium) (7.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from gymnasium[classic-control]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (7.1.0)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\basil\\appdata\\roaming\\python\\python39\\site-packages (from gymnasium[classic-control]) (2.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\program files (x86)\\microsoft visual studio\\shared\\python39_64\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290e9d45-e28b-4603-9058-3260849a1668",
   "metadata": {
    "id": "290e9d45-e28b-4603-9058-3260849a1668"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99242b2c-e34a-4e2b-a0ce-69dc353240c2",
   "metadata": {
    "id": "99242b2c-e34a-4e2b-a0ce-69dc353240c2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# MountainCar Deep Q-Learning\n",
    "class MountainCarDQL():\n",
    "    # Hyperparameters (adjustable)\n",
    "    learning_rate_a = 0.01         # learning rate (alpha)\n",
    "    discount_factor_g = 0.9         # discount rate (gamma)\n",
    "    network_sync_rate = 50000          # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 100000       # size of replay memory\n",
    "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "    num_divisions = 20\n",
    "\n",
    "    # Neural Network\n",
    "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "\n",
    "    # Train the environment\n",
    "    def train(self, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count=0\n",
    "        goal_reached=False\n",
    "        best_rewards=-200\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "\n",
    "            rewards = 0\n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and rewards>-1000):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    # select best action\n",
    "                    with torch.no_grad():\n",
    "                        #print(policy_dqn(self.state_to_dqn_input(state)))\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "                # Accumulate reward\n",
    "                rewards += reward\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count+=1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            rewards_per_episode.append(rewards)\n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                # Save policy\n",
    "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size:# and goal_reached:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count=0\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
    "        # Create new graph\n",
    "        plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
    "        # for x in range(len(rewards_per_episode)):\n",
    "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        # plt.plot(sum_rewards)\n",
    "        plt.plot(rewards_per_episode)\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        # Save plots\n",
    "        plt.savefig('mountaincar_dql.png')\n",
    "    # Optimize policy network\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated:\n",
    "                # Agent receive reward of 0 for reaching goal.\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value\n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "        #print(loss)\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    '''\n",
    "    Converts a state (position, velocity) to tensor representation.\n",
    "    Example:\n",
    "    Input = (0.3, -0.03)\n",
    "    Return = tensor([16, 6])\n",
    "    '''\n",
    "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "\n",
    "        return torch.FloatTensor([state_p, state_v])\n",
    "\n",
    "    # Run the environment with the learned policy\n",
    "    def test(self, episodes, model_filepath):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "                # Select best action\n",
    "                with torch.no_grad():\n",
    "                    var = policy_dqn(self.state_to_dqn_input(state))\n",
    "                    #print(var)\n",
    "                    action = var.argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "     # Run the environment with the learned policy\n",
    "    def print(self, model_filepath):\n",
    "       \n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=2, h1_nodes=10, out_actions=3)\n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        print(policy_dqn)\n",
    "        print(policy_dqn.fc1.weight) # = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        print(policy_dqn.out.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f045c962-6024-43b8-8527-1376ce918872",
   "metadata": {
    "id": "f045c962-6024-43b8-8527-1376ce918872",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mountaincar = MountainCarDQL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "id": "8c55590b-3de2-4e5e-a9dc-10c7b25b598d",
    "outputId": "5d51151e-415f-4612-c064-a8a6a337d5d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mountaincar.train(25000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
   "metadata": {
    "id": "71956c2f-0661-4b8c-bc5d-245dd9dfe786",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmountaincar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmountaincar_dql_23431.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 243\u001b[0m, in \u001b[0;36mMountainCarDQL.test\u001b[1;34m(self, episodes, model_filepath)\u001b[0m\n\u001b[0;32m    240\u001b[0m             action \u001b[38;5;241m=\u001b[39m var\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;66;03m# Execute action\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m         state,reward,terminated,truncated,_ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:148\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m (position, velocity)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mountaincar.test(10, \"mountaincar_dql_23431.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de7f5e5-a80c-4f80-8342-2327a0de9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.1692, -0.5422],\n",
      "        [-0.5403, -0.1118],\n",
      "        [-0.7500, -0.0213],\n",
      "        [-0.7580,  0.0218],\n",
      "        [ 0.0900,  0.5659],\n",
      "        [ 0.0203,  0.6060],\n",
      "        [-0.2351, -0.3665],\n",
      "        [-0.0348, -0.7156],\n",
      "        [-0.1158, -0.5073],\n",
      "        [-0.5872,  0.0215]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3058,  0.2379, -0.0113, -0.1135,  1.1597, -0.9462, -0.0374,  0.2266,\n",
      "         -0.2753, -0.2412],\n",
      "        [ 0.1222,  0.1303,  0.1516,  0.0678,  1.1755, -0.9546, -0.1078,  0.0779,\n",
      "         -0.0402,  0.0219],\n",
      "        [-0.2718,  0.3103,  0.1038,  0.0866,  1.3623, -1.0175, -0.0760,  0.2576,\n",
      "         -0.1153,  0.0337]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "mountaincar.print(\"mountaincar_dql_23431.pt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
