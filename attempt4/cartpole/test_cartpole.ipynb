{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399964ac-c519-4f5c-a64a-01223d852e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e216dd36-9778-44c2-9c62-5f2bf38b3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# MountainCar Deep Q-Learning\n",
    "class MountainCarDQL():\n",
    "    # Hyperparameters (adjustable)\n",
    "    learning_rate_a = 0.2         # learning rate (alpha)\n",
    "    discount_factor_g = 0.95         # discount rate (gamma)\n",
    "    network_sync_rate = 50000          # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 100000       # size of replay memory\n",
    "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "    num_divisions = 40\n",
    "\n",
    "    # Neural Network\n",
    "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "\n",
    "    # Train the environment\n",
    "    def train(self, episodes, render=False):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('CartPole-v0', render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.shape[0] # expecting 2: position & velocity\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "        self.pos_space1 = np.linspace(env.observation_space.low[2], env.observation_space.high[2], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space1 = np.linspace(env.observation_space.low[3], env.observation_space.high[3], self.num_divisions)\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=24, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=24, out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count=0\n",
    "        goal_reached=False\n",
    "        best_rewards=-200\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False\n",
    "            rewards = 0\n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    # select best action\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "                # Accumulate reward\n",
    "                rewards += reward\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count+=1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            rewards_per_episode.append(rewards)\n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                # Save policy\n",
    "                torch.save(policy_dqn.state_dict(), f\"CartPole_dql_{i}.pt\")\n",
    "\n",
    "            # Check if enough experience has been collected\n",
    "            if len(memory)>self.mini_batch_size and goal_reached:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon*0.99999, 0.05)\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count=0\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "\n",
    "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
    "        # Create new graph\n",
    "        plt.figure(1)\n",
    "\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        # plt.plot(sum_rewards)\n",
    "        plt.plot(rewards_per_episode)\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        # Save plots\n",
    "        plt.savefig('mountaincar_dql.png')\n",
    "    # Optimize policy network\n",
    "\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated:\n",
    "                # Agent receive reward of 0 for reaching goal.\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value\n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        state_p1 = np.digitize(state[2], self.pos_space1)\n",
    "        state_v1 = np.digitize(state[3], self.vel_space1)\n",
    "\n",
    "        return torch.FloatTensor([state_p, state_v, state_p1, state_v1])\n",
    "\n",
    "    # Run the environment with the learned policy\n",
    "    def test(self, episodes, model_filepath):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('CartPole-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "        self.pos_space1 = np.linspace(env.observation_space.low[2], env.observation_space.high[2], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space1 = np.linspace(env.observation_space.low[3], env.observation_space.high[3], self.num_divisions)\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=24, out_actions=num_actions)\n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "                # Select best action\n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0a8fe6-d508-46c3-92a0-96d4be1f5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basil\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "mountaincar = MountainCarDQL()\n",
    "mountaincar.test(10, 'CartPole_dql_3590.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
